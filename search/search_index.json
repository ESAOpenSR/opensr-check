{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"A comprehensive benchmark for real-world Sentinel-2 imagery super-resolution GitHub : https://github.com/ESAOpenSR/opensr-test Documentation : https://esaopensr.github.io/opensr-test PyPI : https://pypi.org/project/opensr-test/ Paper : https://www.techrxiv.org/users/760184/articles/735467-a-comprehensive-benchmark-for-optical-remote-sensing-image-super-resolution Overview Super-Resolution (SR) aims to improve satellite imagery ground sampling distance. However, two problems are common in the literature. First, most models are tested on synthetic data , raising doubts about their real-world applicability and performance. Second, traditional evaluation metrics such as PSNR, LPIPS, and SSIM are not designed to assess SR performance. These metrics fall short, especially in conditions involving changes in luminance or spatial misalignments - scenarios frequently encountered in real world. To address these challenges, 'opensr-test' provides a fair approach for SR benchmark. We provide three datasets carefully crafted to minimize spatial and spectral misalignment. Besides, 'opensr-test' precisely assesses SR algorithm performance across three independent metrics groups that measure consistency, synthesis, and correctness. How to use The example below shows how to use opensr-test to benchmark your SR model. import torch import opensr_test lr = torch . rand ( 4 , 64 , 64 ) hr = torch . rand ( 4 , 256 , 256 ) sr = torch . rand ( 4 , 256 , 256 ) metrics = opensr_test . Metrics () metrics . compute ( lr = lr , sr = sr , hr = hr ) >>> { 'reflectance' : 0.253 , 'spectral' : 26.967 , 'spatial' : 0.0 , 'synthesis' : 0.2870 , 'ha_percent' : 0.892 , 'om_percent' : 0.0613 , 'im_percent' : 0.04625 } This model returns: reflectance : How SR affects the reflectance values of the LR image. By default, it uses the L1 norm. The lower the value, the better the reflectance consistency. spectral : How SR affects the spectral signature of the LR image. By default, it uses the spectral angle distance (SAM). The lower the value, the better the spectral consistency. The angles are in degrees. spatial : The spatial alignment between the SR and LR images. By default, it uses Phase Correlation Coefficient (PCC). Some SR models introduce spatial shift, which can be detected by this metric. synthesis : The high-frequency details introduced by the SR model. By default, it uses the L1 norm. The lower the value, the better the synthesis quality. ha_percent : The percentage of pixels in the SR image that are classified as hallucinations. A hallucination is a detail in the SR image that is not present in the HR image. om_percent : The percentage of pixels in the SR image that are classified as omissions. An omission is a detail in the HR image that is not present in the SR image. im_percent : The percentage of pixels in the SR image that are classified as improvements. An improvement is a detail in the SR image that is present in the HR image and not in the LR image. Benchmark Benchmark comparison of SR models. Downward arrows (\u2193) denote metrics in which lower values are preferable, and upward arrows (\u2191) indicate metrics in which higher values reflect better performance. Installation Install the latest version from PyPI: pip install opensr-test Upgrade opensr-test by running: pip install -U opensr-test Install the latest dev version from GitHub by running: pip install git+https://github.com/ESAOpenSR/opensr-test Datasets The opensr-test package provides five datasets for benchmarking SR models. These datasets are carefully crafted to minimize spatial and spectral misalignment. See our Hugging Face repository for more details about the datasets. https://huggingface.co/datasets/isp-uv-es/opensr-test NAIP (X4 scale factor) The National Agriculture Imagery Program (NAIP) dataset is a high-resolution aerial imagery dataset that covers the continental United States. The dataset consists of 2.5m NAIP imagery captured in the visible and near-infrared spectrum (RGBNIR) and all Sentinel-2 L1C and L2A bands. The dataset focus in crop fields, forests, and bare soil areas . import opensr_test naip = opensr_test . load ( \"naip\" ) SPOT (X4 scale factor) The SPOT imagery were obtained from the worldstat dataset. The dataset consists of 2.5m SPOT imagery captured in the visible and near-infrared spectrum (RGBNIR) and all Sentinel-2 L1C and L2A bands. The dataset focus in urban areas, crop fields, and bare soil areas . import opensr_test spot = opensr_test . load ( \"spot\" ) Ven\u00b5s (X2 scale factor) The Ven\u00b5s images were obtained from the Sen2Ven\u00b5s dataset . The dataset consists of 5m Ven\u00b5s imagery captured in the visible and near-infrared spectrum (RGBNIR) and all Sentinel-2 L1C and L2A bands. The dataset focus in crop fields, forests, urban areas, and bare soil areas . import opensr_test venus = opensr_test . load ( \"venus\" ) SPAIN CROPS (x4 scale factor) The SPAIN CROPS dataset consists of 2.5m aerial imagery captured in the visible and near-infrared spectrum (RGBNIR) by the Spanish National Geographic Institute (IGN). The dataset includes all Sentinel-2 L1C and L2A bands. The dataset focus in crop fields and forests . import opensr_test spain_crops = opensr_test . load ( \"spain_crops\" ) SPAIN URBAN (x4 scale factor) The SPAIN URBAN dataset consists of 2.5m aerial imagery captured in the visible and near-infrared spectrum (RGBNIR) by the Spanish National Geographic Institute (IGN). The dataset includes all Sentinel-2 L1C and L2A bands. The dataset focus in urban areas . spain_urban = opensr_test . load ( \"spain_urban\" ) Examples The following examples show how to use opensr-test to benchmark your SR model. Use opensr-test with TensorFlow model (SR4RS) Use opensr-test with PyTorch model (SuperImage) Use opensr-test with a diffuser model (LDMSuperResolutionPipeline) Use opensr-test with a diffuser model (opensr-model) Use opensr-test with Pytorch (EvoLand) Use opensr-test with Pytorch (SWIN2-MOSE) Use opensr-test with Pytorch (synthetic dataset) Visualizations The opensr-test package provides a set of visualizations to help you understand the performance of your SR model. import torch import opensr_test import matplotlib.pyplot as plt from super_image import HanModel # Define the SR model srmodel = HanModel . from_pretrained ( 'eugenesiow/han' , scale = 4 ) # Load the data lr , hr , parameters = opensr_test . load ( \"spot\" ) . values () # Define the benchmark experiment metrics = opensr_test . Metrics () # Define the image to be tested idx = 0 lr_img = torch . from_numpy ( lr [ idx , 0 : 3 ]) hr_img = torch . from_numpy ( hr [ idx , 0 : 3 ]) sr_img = srmodel ( lr_img [ None ]) . squeeze () . detach () # Compute the metrics metrics . compute ( lr = lr_img , sr = sr_img , hr = hr_img , gradient_threshold = parameters [ idx ] ) Now, we can visualize the results using the opensr_test.visualize module. fDisplay the triplets LR, SR and HR images: metrics . plot_triplets () Display a summary of all the metrics: metrics . plot_summary () Display the correctness of the SR image: metrics . plot_tc () Deeper understanding Explore the API section for more details about personalizing your benchmark experiments. Citation If you use opensr-test in your research, please cite our paper: @article{aybar2024comprehensive, title={A Comprehensive Benchmark for Optical Remote Sensing Image Super-Resolution}, author={Aybar, Cesar and Montero, David and Donike, Simon and Kalaitzis, Freddie and G{\\'o}mez-Chova, Luis}, journal={Authorea Preprints}, year={2024}, publisher={Authorea} } Acknowledgements This work was make with the support of the European Space Agency (ESA) under the project \u201cExplainable AI: application to trustworthy super-resolution (OpenSR)\u201d. Cesar Aybar acknowledges support by the National Council of Science, Technology, and Technological Innovation (CONCYTEC, Peru) through the \u201cPROYECTOS DE INVESTIGACI\u00d3N B\u00c1SICA \u2013 2023-01\u201d program with contract number PE501083135-2023-PROCIENCIA. Luis G\u00f3mez-Chova acknowledges support from the Spanish Ministry of Science and Innovation (project PID2019-109026RB-I00 funded by MCIN/AEI/10.13039/501100011033).","title":"Index"},{"location":"index.html#_1","text":"","title":""},{"location":"index.html#overview","text":"Super-Resolution (SR) aims to improve satellite imagery ground sampling distance. However, two problems are common in the literature. First, most models are tested on synthetic data , raising doubts about their real-world applicability and performance. Second, traditional evaluation metrics such as PSNR, LPIPS, and SSIM are not designed to assess SR performance. These metrics fall short, especially in conditions involving changes in luminance or spatial misalignments - scenarios frequently encountered in real world. To address these challenges, 'opensr-test' provides a fair approach for SR benchmark. We provide three datasets carefully crafted to minimize spatial and spectral misalignment. Besides, 'opensr-test' precisely assesses SR algorithm performance across three independent metrics groups that measure consistency, synthesis, and correctness.","title":"Overview"},{"location":"index.html#how-to-use","text":"The example below shows how to use opensr-test to benchmark your SR model. import torch import opensr_test lr = torch . rand ( 4 , 64 , 64 ) hr = torch . rand ( 4 , 256 , 256 ) sr = torch . rand ( 4 , 256 , 256 ) metrics = opensr_test . Metrics () metrics . compute ( lr = lr , sr = sr , hr = hr ) >>> { 'reflectance' : 0.253 , 'spectral' : 26.967 , 'spatial' : 0.0 , 'synthesis' : 0.2870 , 'ha_percent' : 0.892 , 'om_percent' : 0.0613 , 'im_percent' : 0.04625 } This model returns: reflectance : How SR affects the reflectance values of the LR image. By default, it uses the L1 norm. The lower the value, the better the reflectance consistency. spectral : How SR affects the spectral signature of the LR image. By default, it uses the spectral angle distance (SAM). The lower the value, the better the spectral consistency. The angles are in degrees. spatial : The spatial alignment between the SR and LR images. By default, it uses Phase Correlation Coefficient (PCC). Some SR models introduce spatial shift, which can be detected by this metric. synthesis : The high-frequency details introduced by the SR model. By default, it uses the L1 norm. The lower the value, the better the synthesis quality. ha_percent : The percentage of pixels in the SR image that are classified as hallucinations. A hallucination is a detail in the SR image that is not present in the HR image. om_percent : The percentage of pixels in the SR image that are classified as omissions. An omission is a detail in the HR image that is not present in the SR image. im_percent : The percentage of pixels in the SR image that are classified as improvements. An improvement is a detail in the SR image that is present in the HR image and not in the LR image.","title":"How to use"},{"location":"index.html#benchmark","text":"Benchmark comparison of SR models. Downward arrows (\u2193) denote metrics in which lower values are preferable, and upward arrows (\u2191) indicate metrics in which higher values reflect better performance.","title":"Benchmark"},{"location":"index.html#installation","text":"Install the latest version from PyPI: pip install opensr-test Upgrade opensr-test by running: pip install -U opensr-test Install the latest dev version from GitHub by running: pip install git+https://github.com/ESAOpenSR/opensr-test","title":"Installation"},{"location":"index.html#datasets","text":"The opensr-test package provides five datasets for benchmarking SR models. These datasets are carefully crafted to minimize spatial and spectral misalignment. See our Hugging Face repository for more details about the datasets. https://huggingface.co/datasets/isp-uv-es/opensr-test","title":"Datasets"},{"location":"index.html#naip-x4-scale-factor","text":"The National Agriculture Imagery Program (NAIP) dataset is a high-resolution aerial imagery dataset that covers the continental United States. The dataset consists of 2.5m NAIP imagery captured in the visible and near-infrared spectrum (RGBNIR) and all Sentinel-2 L1C and L2A bands. The dataset focus in crop fields, forests, and bare soil areas . import opensr_test naip = opensr_test . load ( \"naip\" )","title":"NAIP (X4 scale factor)"},{"location":"index.html#spot-x4-scale-factor","text":"The SPOT imagery were obtained from the worldstat dataset. The dataset consists of 2.5m SPOT imagery captured in the visible and near-infrared spectrum (RGBNIR) and all Sentinel-2 L1C and L2A bands. The dataset focus in urban areas, crop fields, and bare soil areas . import opensr_test spot = opensr_test . load ( \"spot\" )","title":"SPOT (X4 scale factor)"},{"location":"index.html#vens-x2-scale-factor","text":"The Ven\u00b5s images were obtained from the Sen2Ven\u00b5s dataset . The dataset consists of 5m Ven\u00b5s imagery captured in the visible and near-infrared spectrum (RGBNIR) and all Sentinel-2 L1C and L2A bands. The dataset focus in crop fields, forests, urban areas, and bare soil areas . import opensr_test venus = opensr_test . load ( \"venus\" )","title":"Ven\u00b5s (X2 scale factor)"},{"location":"index.html#spain-crops-x4-scale-factor","text":"The SPAIN CROPS dataset consists of 2.5m aerial imagery captured in the visible and near-infrared spectrum (RGBNIR) by the Spanish National Geographic Institute (IGN). The dataset includes all Sentinel-2 L1C and L2A bands. The dataset focus in crop fields and forests . import opensr_test spain_crops = opensr_test . load ( \"spain_crops\" )","title":"SPAIN CROPS (x4 scale factor)"},{"location":"index.html#spain-urban-x4-scale-factor","text":"The SPAIN URBAN dataset consists of 2.5m aerial imagery captured in the visible and near-infrared spectrum (RGBNIR) by the Spanish National Geographic Institute (IGN). The dataset includes all Sentinel-2 L1C and L2A bands. The dataset focus in urban areas . spain_urban = opensr_test . load ( \"spain_urban\" )","title":"SPAIN URBAN (x4 scale factor)"},{"location":"index.html#examples","text":"The following examples show how to use opensr-test to benchmark your SR model. Use opensr-test with TensorFlow model (SR4RS) Use opensr-test with PyTorch model (SuperImage) Use opensr-test with a diffuser model (LDMSuperResolutionPipeline) Use opensr-test with a diffuser model (opensr-model) Use opensr-test with Pytorch (EvoLand) Use opensr-test with Pytorch (SWIN2-MOSE) Use opensr-test with Pytorch (synthetic dataset)","title":"Examples"},{"location":"index.html#visualizations","text":"The opensr-test package provides a set of visualizations to help you understand the performance of your SR model. import torch import opensr_test import matplotlib.pyplot as plt from super_image import HanModel # Define the SR model srmodel = HanModel . from_pretrained ( 'eugenesiow/han' , scale = 4 ) # Load the data lr , hr , parameters = opensr_test . load ( \"spot\" ) . values () # Define the benchmark experiment metrics = opensr_test . Metrics () # Define the image to be tested idx = 0 lr_img = torch . from_numpy ( lr [ idx , 0 : 3 ]) hr_img = torch . from_numpy ( hr [ idx , 0 : 3 ]) sr_img = srmodel ( lr_img [ None ]) . squeeze () . detach () # Compute the metrics metrics . compute ( lr = lr_img , sr = sr_img , hr = hr_img , gradient_threshold = parameters [ idx ] ) Now, we can visualize the results using the opensr_test.visualize module. fDisplay the triplets LR, SR and HR images: metrics . plot_triplets () Display a summary of all the metrics: metrics . plot_summary () Display the correctness of the SR image: metrics . plot_tc ()","title":"Visualizations"},{"location":"index.html#deeper-understanding","text":"Explore the API section for more details about personalizing your benchmark experiments.","title":"Deeper understanding"},{"location":"index.html#citation","text":"If you use opensr-test in your research, please cite our paper: @article{aybar2024comprehensive, title={A Comprehensive Benchmark for Optical Remote Sensing Image Super-Resolution}, author={Aybar, Cesar and Montero, David and Donike, Simon and Kalaitzis, Freddie and G{\\'o}mez-Chova, Luis}, journal={Authorea Preprints}, year={2024}, publisher={Authorea} }","title":"Citation"},{"location":"index.html#acknowledgements","text":"This work was make with the support of the European Space Agency (ESA) under the project \u201cExplainable AI: application to trustworthy super-resolution (OpenSR)\u201d. Cesar Aybar acknowledges support by the National Council of Science, Technology, and Technological Innovation (CONCYTEC, Peru) through the \u201cPROYECTOS DE INVESTIGACI\u00d3N B\u00c1SICA \u2013 2023-01\u201d program with contract number PE501083135-2023-PROCIENCIA. Luis G\u00f3mez-Chova acknowledges support from the Spanish Ministry of Science and Innovation (project PID2019-109026RB-I00 funded by MCIN/AEI/10.13039/501100011033).","title":"Acknowledgements"},{"location":"CONTRIBUTING.html","text":"Contributing to opensr-test Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions Report Bugs Report bugs at https://github.com/csaybar/opensr-test/issues If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement a fix for it. Implement Features Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation Cookiecutter PyPackage could always use more documentation, whether as part of the official docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback The best way to send feedback is to file an issue at https://github.com/csaybar/opensr-test/issues. If you are proposing a new feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! Ready to contribute? Here's how to set up opensr-test for local development. Please note this documentation assumes you already have poetry and Git installed and ready to go. Fork the opensr-test repo on GitHub. Clone your fork locally: cd <directory_in_which_repo_should_be_created> git clone git@github.com:YOUR_NAME/opensr-test.git Now we need to install the environment. Navigate into the directory cd opensr-test If you are using pyenv , select a version to use locally. (See installed versions with pyenv versions ) pyenv local <x.y.z> Then, install and activate the environment with: poetry install poetry shell Install pre-commit to run linters/formatters at commit time: poetry run pre-commit install Create a branch for local development: git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. Don't forget to add test cases for your added functionality to the tests directory. When you're done making changes, check that your changes pass the formatting tests. make check Now, validate that all unit tests are passing: make test Before raising a pull request you should also run tox. This will run the tests across different versions of Python: tox This requires you to have multiple versions of python installed. This step is also triggered in the CI/CD pipeline, so you could also choose to skip this step locally. Commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md .","title":"Contributing to `opensr-test`"},{"location":"CONTRIBUTING.html#contributing-to-opensr-test","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing to opensr-test"},{"location":"CONTRIBUTING.html#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"CONTRIBUTING.html#report-bugs","text":"Report bugs at https://github.com/csaybar/opensr-test/issues If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"CONTRIBUTING.html#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement a fix for it.","title":"Fix Bugs"},{"location":"CONTRIBUTING.html#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"CONTRIBUTING.html#write-documentation","text":"Cookiecutter PyPackage could always use more documentation, whether as part of the official docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"CONTRIBUTING.html#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/csaybar/opensr-test/issues. If you are proposing a new feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"CONTRIBUTING.html#get-started","text":"Ready to contribute? Here's how to set up opensr-test for local development. Please note this documentation assumes you already have poetry and Git installed and ready to go. Fork the opensr-test repo on GitHub. Clone your fork locally: cd <directory_in_which_repo_should_be_created> git clone git@github.com:YOUR_NAME/opensr-test.git Now we need to install the environment. Navigate into the directory cd opensr-test If you are using pyenv , select a version to use locally. (See installed versions with pyenv versions ) pyenv local <x.y.z> Then, install and activate the environment with: poetry install poetry shell Install pre-commit to run linters/formatters at commit time: poetry run pre-commit install Create a branch for local development: git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. Don't forget to add test cases for your added functionality to the tests directory. When you're done making changes, check that your changes pass the formatting tests. make check Now, validate that all unit tests are passing: make test Before raising a pull request you should also run tox. This will run the tests across different versions of Python: tox This requires you to have multiple versions of python installed. This step is also triggered in the CI/CD pipeline, so you could also choose to skip this step locally. Commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"CONTRIBUTING.html#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md .","title":"Pull Request Guidelines"},{"location":"docs/CHANGELOG.html","text":"Changelog All notable changes to the opensr-test project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Added For new features. Changed For changes in existing functionality. Deprecated For soon-to-be removed features. Removed For now removed features. Fixed For any bug fixes. Security In case of vulnerabilities. Unreleased Unreleased changes here. [1.0.0] - 2024-05-20 Logo changed. The lightglue submodule has been replaced with a new package called satalign . Two new models have been added to the benchmark: EvoLand and SWIN2-MOSE . A new example has been added on how to run opensr-test on synthetic data. The documentation has been updated. The harmonization module has been updated to include the satalign package. We have added two new datasets: spain_crops and spain_urban . A nex example about how to run all the metrics simultaneously has been added. [0.2.0] - 2023-12-20 Paper submited to the IEEE remote sensing letters. 0.1.0 - 2023-12-20 Added First release of the opensr-test package.","title":"Changelog"},{"location":"docs/CHANGELOG.html#changelog","text":"All notable changes to the opensr-test project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"docs/CHANGELOG.html#added","text":"For new features.","title":"Added"},{"location":"docs/CHANGELOG.html#changed","text":"For changes in existing functionality.","title":"Changed"},{"location":"docs/CHANGELOG.html#deprecated","text":"For soon-to-be removed features.","title":"Deprecated"},{"location":"docs/CHANGELOG.html#removed","text":"For now removed features.","title":"Removed"},{"location":"docs/CHANGELOG.html#fixed","text":"For any bug fixes.","title":"Fixed"},{"location":"docs/CHANGELOG.html#security","text":"In case of vulnerabilities.","title":"Security"},{"location":"docs/CHANGELOG.html#unreleased","text":"Unreleased changes here.","title":"Unreleased"},{"location":"docs/CHANGELOG.html#100-2024-05-20","text":"Logo changed. The lightglue submodule has been replaced with a new package called satalign . Two new models have been added to the benchmark: EvoLand and SWIN2-MOSE . A new example has been added on how to run opensr-test on synthetic data. The documentation has been updated. The harmonization module has been updated to include the satalign package. We have added two new datasets: spain_crops and spain_urban . A nex example about how to run all the metrics simultaneously has been added.","title":"[1.0.0] - 2024-05-20"},{"location":"docs/CHANGELOG.html#020-2023-12-20","text":"Paper submited to the IEEE remote sensing letters.","title":"[0.2.0] - 2023-12-20"},{"location":"docs/CHANGELOG.html#010-2023-12-20","text":"","title":"0.1.0 - 2023-12-20"},{"location":"docs/CHANGELOG.html#added_1","text":"First release of the opensr-test package.","title":"Added"},{"location":"docs/CONTRIBUTING.html","text":"Contributing to opensr-test We love your input! We want to make contributing to this project as easy and transparent as possible, whether it's: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Becoming a maintainer We Develop with Github We use GitHub to host code, to track issues and feature requests, as well as accept pull requests. Pull requests are the best way to propose changes to the codebase. We actively welcome your pull requests: Fork the repo and create your branch from main . If you've added code that should be tested, add tests. If you've changed APIs, update the documentation. Ensure the test suite passes. Make sure your code lints. Issue that pull request! Any contributions you make will be under the MIT Software License In short, when you submit code changes, your submissions are understood to be under the same MIT License that covers the project. Feel free to contact the maintainers if that's a concern. Report bugs using Github's issues We use GitHub issues to track public bugs. Report a bug by opening a new issue ; it's that easy! Write bug reports with detail, background, and sample code Great Bug Reports tend to have: A quick summary and/or background Steps to reproduce Be specific! Give sample code if you can. What you expected would happen What actually happens Notes (possibly including why you think this might be happening, or stuff you tried that didn't work) People love thorough bug reports. Use a Consistent Coding Style 4 spaces for indentation rather than tabs Follow PEP8 where possible. If possible, use Black to format your code. Add comments to your code where necessary. In Python code, use docstrings. License By contributing, you agree that your contributions will be licensed under its MIT License.","title":"Contributing"},{"location":"docs/CONTRIBUTING.html#contributing-to-opensr-test","text":"We love your input! We want to make contributing to this project as easy and transparent as possible, whether it's: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Becoming a maintainer","title":"Contributing to opensr-test"},{"location":"docs/CONTRIBUTING.html#we-develop-with-github","text":"We use GitHub to host code, to track issues and feature requests, as well as accept pull requests. Pull requests are the best way to propose changes to the codebase. We actively welcome your pull requests: Fork the repo and create your branch from main . If you've added code that should be tested, add tests. If you've changed APIs, update the documentation. Ensure the test suite passes. Make sure your code lints. Issue that pull request!","title":"We Develop with Github"},{"location":"docs/CONTRIBUTING.html#any-contributions-you-make-will-be-under-the-mit-software-license","text":"In short, when you submit code changes, your submissions are understood to be under the same MIT License that covers the project. Feel free to contact the maintainers if that's a concern.","title":"Any contributions you make will be under the MIT Software License"},{"location":"docs/CONTRIBUTING.html#report-bugs-using-githubs-issues","text":"We use GitHub issues to track public bugs. Report a bug by opening a new issue ; it's that easy!","title":"Report bugs using Github's issues"},{"location":"docs/CONTRIBUTING.html#write-bug-reports-with-detail-background-and-sample-code","text":"Great Bug Reports tend to have: A quick summary and/or background Steps to reproduce Be specific! Give sample code if you can. What you expected would happen What actually happens Notes (possibly including why you think this might be happening, or stuff you tried that didn't work) People love thorough bug reports.","title":"Write bug reports with detail, background, and sample code"},{"location":"docs/CONTRIBUTING.html#use-a-consistent-coding-style","text":"4 spaces for indentation rather than tabs Follow PEP8 where possible. If possible, use Black to format your code. Add comments to your code where necessary. In Python code, use docstrings.","title":"Use a Consistent Coding Style"},{"location":"docs/CONTRIBUTING.html#license","text":"By contributing, you agree that your contributions will be licensed under its MIT License.","title":"License"},{"location":"docs/Code_of_Conduct.html","text":"Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [cesar.aybar@uv.es]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct/code_of_conduct.md For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"docs/Code_of_Conduct.html#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"docs/Code_of_Conduct.html#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"docs/Code_of_Conduct.html#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"docs/Code_of_Conduct.html#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"docs/Code_of_Conduct.html#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"docs/Code_of_Conduct.html#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [cesar.aybar@uv.es]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"docs/Code_of_Conduct.html#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct/code_of_conduct.md For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"docs/LICENSE.html","text":"The MIT License (MIT) Copyright (c) 2023 OpenSR team Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"docs/API/compute_method.html","text":"The compute Method The compute method calculate the suite of metrics in the opensr-test framework. This method includes three mandatory parameters, and one optional parameter: lr ( torch.Tensor ): The Low Resolution (LR) image tensor. The shape of the tensor should be (channel, height/n, width/n) . Where n is the scaling factor of the Super-Resolution (SR) image. sr ( torch.Tensor ): The Super-Resolved (SR) image tensor generated by the model under evaluation. The shape of the tensor should be (channel, height, width) . hr ( torch.Tensor ): The High Resolution (HR) image tensor (ground truth). The shape of the tensor should be (channel, height, width) . gradient_threshold (default=0.01): Ignore the pixels with gradients below this threshold. This parameter is used to filter out pixels with insignificant differences between the SR and HR images. The default value is set to 0.01, which is optimized for balanced assessment. However, this parameter can be adjusted to each image. Based on the expert judgment of three remote sensing specialists, the optimal value of this parameter has been determined for each image within the opensr-test datasets, allowing for a more tailored and precise evaluation for each specific dataset. Below is an example that demonstrates the usage of the compute method with the aforementioned parameters: # Import necessary libraries import torch import opensr_test import matplotlib.pyplot as plt # Generate sample LR, HR, and SR images lr = torch . rand ( 4 , 32 , 32 ) # Low Resolution image hr = torch . rand ( 4 , 256 , 256 ) # High Resolution image sr = torch . rand ( 4 , 256 , 256 ) # Super Resolution image # Initialize the Metrics object metrics = opensr_test . Metrics () # Compute the metrics with specified parameters metrics . compute ( lr = lr , sr = sr , hr = hr , stability_threshold = 0.03 )","title":"Compute method"},{"location":"docs/API/compute_method.html#_1","text":"","title":""},{"location":"docs/API/compute_method.html#the-compute-method","text":"The compute method calculate the suite of metrics in the opensr-test framework. This method includes three mandatory parameters, and one optional parameter: lr ( torch.Tensor ): The Low Resolution (LR) image tensor. The shape of the tensor should be (channel, height/n, width/n) . Where n is the scaling factor of the Super-Resolution (SR) image. sr ( torch.Tensor ): The Super-Resolved (SR) image tensor generated by the model under evaluation. The shape of the tensor should be (channel, height, width) . hr ( torch.Tensor ): The High Resolution (HR) image tensor (ground truth). The shape of the tensor should be (channel, height, width) . gradient_threshold (default=0.01): Ignore the pixels with gradients below this threshold. This parameter is used to filter out pixels with insignificant differences between the SR and HR images. The default value is set to 0.01, which is optimized for balanced assessment. However, this parameter can be adjusted to each image. Based on the expert judgment of three remote sensing specialists, the optimal value of this parameter has been determined for each image within the opensr-test datasets, allowing for a more tailored and precise evaluation for each specific dataset. Below is an example that demonstrates the usage of the compute method with the aforementioned parameters: # Import necessary libraries import torch import opensr_test import matplotlib.pyplot as plt # Generate sample LR, HR, and SR images lr = torch . rand ( 4 , 32 , 32 ) # Low Resolution image hr = torch . rand ( 4 , 256 , 256 ) # High Resolution image sr = torch . rand ( 4 , 256 , 256 ) # Super Resolution image # Initialize the Metrics object metrics = opensr_test . Metrics () # Compute the metrics with specified parameters metrics . compute ( lr = lr , sr = sr , hr = hr , stability_threshold = 0.03 )","title":"The compute Method"},{"location":"docs/API/config_pydantic.html","text":"Config The Config class is a pydantic.BaseModel that defines model parameters. The parameters are categorized into four groups, each catering to different aspects of the super-resolution (SR) evaluation process: Global Parameters: These parameters control the overall behavior of the SR assessment: device: Selects the computation device for inference, defaulting to cpu . cuda is also supported. agg_method: Determines the granularity for distance metric application \u2014 pixel-wise by default (\"pixel\"), with \"patch\" and \"image\" level aggregations options. If \"patch\" is selected, the metric is computed on patches of size patch_size and then interpolated to the original image size using bicubic interpolation with anti-aliasing. If \"image\" is selected, the metric is computed on the entire image and output as a single value. If \"pixel\" is selected, the metric is computed on each pixel and output as a tensor of the same size as the input image. patch_size: Relevant when agg_method is \"patch\", this defines the patch size for distance metric computation. border_mask: Excludes image borders in metric calculations, ignoring the outer 16 pixels by default. rgb_bands: Necessary for certain metrics like LPIPS, CLIP and plot generation, assuming a default order of Red-Green-Blue ([0, 1, 2]). harm_apply_spectral: Applies histogram matching before correctness metrics. This is enabled by default. harm_apply_spatial: Activates spatial alignment before correctness metrics. This is enabled by default. Spatial Parameters: These parameters set the spatial alignment pre-processing: spatial_method: The default method for spatial alignment. By default, pcc (Phase Correlation Coefficient) is used. However, other methods like \"ecc\" (Enhanced Correlation Coefficient) and \"lgm\" (SuperPoint + LightGlue) are also available. ecc is more robust to noise and usually more precise than pcc, but it is slower. lgm is slower than ecc and pcc, but it is more robust to large translations. spatial_threshold_distance : The maximum permissible translation distance for spatial alignment, set to 5 pixels by default. If the translation distance is greater than this threshold, the spatial alignment is skipped. spatial_max_num_keypoints: Only relevant when the spatial method is \"lgm\". This parameter caps the number of keypoints for feature matching at 500 by default. Spectral and reflectance Parameters: These parameters are specific to spectral analysis and alignment: reflectance_distance: The default method for reflectance distance estimation. Reflectance distance measure how the SR image affects the reflectance of the original image. By default, \"l1\" is used. See section Metrics for more details and other options. spectral_distance: The default method for spectral distance calculation. The spectral distance measure how the SR image affects the spectral signature of the original image. By default, \"sam\" is used. See section Metrics for more details and other options. Synthesis parameters synthesis_distance: Specifies the distance metric between SR and LR after harmonization. By default, \"l1\" is used. See section Metrics for more details and other options. Correctness parameters correctness_distance: Specifies the distance metric between harmonized SR, HR and LR. By default, \"l1\" is used. See section Metrics for more details and other options. Below is an illustrative example of how to instantiate opensr-test with user-defined parameters: import torch import opensr_test # Define the Low Resolution (LR), High Resolution (HR), and Super-Resolved (SR) images. lr = torch . rand ( 4 , 64 , 64 ) hr = torch . rand ( 4 , 256 , 256 ) sr = torch . rand ( 4 , 256 , 256 ) # Initialize the Metrics object with custom parameters. config = opensr_test . Config ( device = \"cuda\" , spatial_features = \"ecc\" , harm_apply_spatial = True , harm_apply_spectral = False ) metrics = opensr_test . Metrics ( config ) # Compute the metrics based on the provided images. metrics . compute ( lr = lr , sr = sr , hr = hr )","title":"Config"},{"location":"docs/API/config_pydantic.html#_1","text":"","title":""},{"location":"docs/API/config_pydantic.html#config","text":"The Config class is a pydantic.BaseModel that defines model parameters. The parameters are categorized into four groups, each catering to different aspects of the super-resolution (SR) evaluation process:","title":"Config"},{"location":"docs/API/config_pydantic.html#global-parameters","text":"These parameters control the overall behavior of the SR assessment: device: Selects the computation device for inference, defaulting to cpu . cuda is also supported. agg_method: Determines the granularity for distance metric application \u2014 pixel-wise by default (\"pixel\"), with \"patch\" and \"image\" level aggregations options. If \"patch\" is selected, the metric is computed on patches of size patch_size and then interpolated to the original image size using bicubic interpolation with anti-aliasing. If \"image\" is selected, the metric is computed on the entire image and output as a single value. If \"pixel\" is selected, the metric is computed on each pixel and output as a tensor of the same size as the input image. patch_size: Relevant when agg_method is \"patch\", this defines the patch size for distance metric computation. border_mask: Excludes image borders in metric calculations, ignoring the outer 16 pixels by default. rgb_bands: Necessary for certain metrics like LPIPS, CLIP and plot generation, assuming a default order of Red-Green-Blue ([0, 1, 2]). harm_apply_spectral: Applies histogram matching before correctness metrics. This is enabled by default. harm_apply_spatial: Activates spatial alignment before correctness metrics. This is enabled by default.","title":"Global Parameters:"},{"location":"docs/API/config_pydantic.html#spatial-parameters","text":"These parameters set the spatial alignment pre-processing: spatial_method: The default method for spatial alignment. By default, pcc (Phase Correlation Coefficient) is used. However, other methods like \"ecc\" (Enhanced Correlation Coefficient) and \"lgm\" (SuperPoint + LightGlue) are also available. ecc is more robust to noise and usually more precise than pcc, but it is slower. lgm is slower than ecc and pcc, but it is more robust to large translations. spatial_threshold_distance : The maximum permissible translation distance for spatial alignment, set to 5 pixels by default. If the translation distance is greater than this threshold, the spatial alignment is skipped. spatial_max_num_keypoints: Only relevant when the spatial method is \"lgm\". This parameter caps the number of keypoints for feature matching at 500 by default.","title":"Spatial Parameters:"},{"location":"docs/API/config_pydantic.html#spectral-and-reflectance-parameters","text":"These parameters are specific to spectral analysis and alignment: reflectance_distance: The default method for reflectance distance estimation. Reflectance distance measure how the SR image affects the reflectance of the original image. By default, \"l1\" is used. See section Metrics for more details and other options. spectral_distance: The default method for spectral distance calculation. The spectral distance measure how the SR image affects the spectral signature of the original image. By default, \"sam\" is used. See section Metrics for more details and other options.","title":"Spectral and reflectance Parameters:"},{"location":"docs/API/config_pydantic.html#synthesis-parameters","text":"synthesis_distance: Specifies the distance metric between SR and LR after harmonization. By default, \"l1\" is used. See section Metrics for more details and other options.","title":"Synthesis parameters"},{"location":"docs/API/config_pydantic.html#correctness-parameters","text":"correctness_distance: Specifies the distance metric between harmonized SR, HR and LR. By default, \"l1\" is used. See section Metrics for more details and other options. Below is an illustrative example of how to instantiate opensr-test with user-defined parameters: import torch import opensr_test # Define the Low Resolution (LR), High Resolution (HR), and Super-Resolved (SR) images. lr = torch . rand ( 4 , 64 , 64 ) hr = torch . rand ( 4 , 256 , 256 ) sr = torch . rand ( 4 , 256 , 256 ) # Initialize the Metrics object with custom parameters. config = opensr_test . Config ( device = \"cuda\" , spatial_features = \"ecc\" , harm_apply_spatial = True , harm_apply_spectral = False ) metrics = opensr_test . Metrics ( config ) # Compute the metrics based on the provided images. metrics . compute ( lr = lr , sr = sr , hr = hr )","title":"Correctness parameters"},{"location":"docs/API/results_attributes.html","text":"The results Attribute The results attribute encapsulates all the metrics at pixel level and the intermediate results produced by the opensr-test test. It organizes the output into four principal categories: consistency , synthesis , correctness , and auxiliary . The following example illustrates how to retrieve the outputs from the results attribute: # Import necessary libraries import torch import opensr_test # Generate sample LR, HR, and SR images lr = torch . rand ( 4 , 64 , 64 ) # Low Resolution image hr = torch . rand ( 4 , 256 , 256 ) # High Resolution image sr = torch . rand ( 4 , 256 , 256 ) # Super Resolution image # Initialize the Metrics object metrics = opensr_test . Metrics () # Compute the metrics metrics . compute ( lr = lr , sr = sr , hr = hr ) # Accessing Consistency Metrics metrics . results . consistency # Accessing Synthesis Metrics metrics . results . synthesis ## Accessing Correctness Metrics metrics . results . correctness # Accessing Intermediate Results metrics . results . auxiliary consistency The consistency metrics within opensr-test play a crucial role in evaluating the harmony between LR and SR images prior to SR harmonization (SRharm). These metrics are calculated after resampling the SR images to match the dimensions of the LR (SRdown). There are three key metrics in this category: reflectance , spectral , and spatial . reflectance This metric evaluates how well the SR image reflects the norm values of the LR image. The calculation of reflectance consistency utilizes the method defined by the reflectance_method parameter. spectral This metric assesses the similarity in spectral characteristics between the LR and SR images. The computation of spectral consistency leverages the angle distance specified in the spectral_method parameter. This allows for a detailed comparison of the spectral profiles of the images, ensuring that the SR image preserves the original spectral properties of the LR image. spatial The spatial consistency metric is computed by calculating, first, the warp affine between the SRdown and LR images. The translation parameters are then extracted from the affine matrix and used to compute the spatial error. synthesis distance The synthesis metrics in opensr-test evaluate the distance between the SRharm and the LR image. The distancec matrix help to understand the high-frequency details introduced by the SR model at local level. Correctness The correctness metrics in opensr-test are crucial assessments conducted after the SR harmonization process and the evaluation of the triple distance. These metrics encompass four categories: improvement , omission , hallucination , and classification . All the correctness metrics are designed such that values closer to 0 indicate that the pixel, patch, or image is nearer to its respective target space. improvement The improvement matrix quantifies the distance to the improvement space. It is a matrix of dimensions HxW. The idel value of the improvement matrix is 0, which indicates that the SR image is identical to the HR image. omission The omission matrix quantifies the distance to the improvement space. It is a matrix of dimensions HxW. A high value in the omission matrix indicates that the SR image has omitted crucial details from the HR image. hallucination The hallucination matrix measures the extent of artificial details or 'hallucinations' introduced in the SR image. It is a matrix of dimensions HxW. A high value in the hallucination matrix indicates that the SR image has introduced artificial details that are not present in the HR image. classification The classification matrix (HxW) is computed by applying a np.argmin function across the aforementioned correctness matrices. This matrix forms the basis for categorizing each pixel into one of the three classes: improvement, omission, and hallucination. auxiliary The auxiliary subset comprises a set of intermediate outputs generated during the execution of the compute method. These results are instrumental in understanding the internal workings and transformations applied during the computation process. There are two auxiliary products: sr_harm , lr_to_hr . sr_harm This is the SR product post-harmonization. The harmonization pipeline is influenced by the harm_apply_spectral and harm_apply_spatial parameters. When both parameters are enabled (set to True, by default), the SR image undergoes a two-step enhancement: first, the reflectance values are corrected via histogram matching with the HR image; subsequently, spatial alignment is performed, aligning the SR image with the HR image based on the method specified in the spatial_method parameter. lr_to_hr This represents the LR image resampled to match the dimensions of the HR image. In the absence of a specific upsample_method set during the setup phase, the lr_to_hr result is achieved using a classic method - bilinear interpolation complemented by an anti-aliasing kernel filter.","title":"Results attributes"},{"location":"docs/API/results_attributes.html#_1","text":"","title":""},{"location":"docs/API/results_attributes.html#the-results-attribute","text":"The results attribute encapsulates all the metrics at pixel level and the intermediate results produced by the opensr-test test. It organizes the output into four principal categories: consistency , synthesis , correctness , and auxiliary . The following example illustrates how to retrieve the outputs from the results attribute: # Import necessary libraries import torch import opensr_test # Generate sample LR, HR, and SR images lr = torch . rand ( 4 , 64 , 64 ) # Low Resolution image hr = torch . rand ( 4 , 256 , 256 ) # High Resolution image sr = torch . rand ( 4 , 256 , 256 ) # Super Resolution image # Initialize the Metrics object metrics = opensr_test . Metrics () # Compute the metrics metrics . compute ( lr = lr , sr = sr , hr = hr ) # Accessing Consistency Metrics metrics . results . consistency # Accessing Synthesis Metrics metrics . results . synthesis ## Accessing Correctness Metrics metrics . results . correctness # Accessing Intermediate Results metrics . results . auxiliary","title":"The results Attribute"},{"location":"docs/API/results_attributes.html#consistency","text":"The consistency metrics within opensr-test play a crucial role in evaluating the harmony between LR and SR images prior to SR harmonization (SRharm). These metrics are calculated after resampling the SR images to match the dimensions of the LR (SRdown). There are three key metrics in this category: reflectance , spectral , and spatial .","title":"consistency"},{"location":"docs/API/results_attributes.html#reflectance","text":"This metric evaluates how well the SR image reflects the norm values of the LR image. The calculation of reflectance consistency utilizes the method defined by the reflectance_method parameter.","title":"reflectance"},{"location":"docs/API/results_attributes.html#spectral","text":"This metric assesses the similarity in spectral characteristics between the LR and SR images. The computation of spectral consistency leverages the angle distance specified in the spectral_method parameter. This allows for a detailed comparison of the spectral profiles of the images, ensuring that the SR image preserves the original spectral properties of the LR image.","title":"spectral"},{"location":"docs/API/results_attributes.html#spatial","text":"The spatial consistency metric is computed by calculating, first, the warp affine between the SRdown and LR images. The translation parameters are then extracted from the affine matrix and used to compute the spatial error.","title":"spatial"},{"location":"docs/API/results_attributes.html#synthesis","text":"","title":"synthesis"},{"location":"docs/API/results_attributes.html#distance","text":"The synthesis metrics in opensr-test evaluate the distance between the SRharm and the LR image. The distancec matrix help to understand the high-frequency details introduced by the SR model at local level.","title":"distance"},{"location":"docs/API/results_attributes.html#correctness","text":"The correctness metrics in opensr-test are crucial assessments conducted after the SR harmonization process and the evaluation of the triple distance. These metrics encompass four categories: improvement , omission , hallucination , and classification . All the correctness metrics are designed such that values closer to 0 indicate that the pixel, patch, or image is nearer to its respective target space.","title":"Correctness"},{"location":"docs/API/results_attributes.html#improvement","text":"The improvement matrix quantifies the distance to the improvement space. It is a matrix of dimensions HxW. The idel value of the improvement matrix is 0, which indicates that the SR image is identical to the HR image.","title":"improvement"},{"location":"docs/API/results_attributes.html#omission","text":"The omission matrix quantifies the distance to the improvement space. It is a matrix of dimensions HxW. A high value in the omission matrix indicates that the SR image has omitted crucial details from the HR image.","title":"omission"},{"location":"docs/API/results_attributes.html#hallucination","text":"The hallucination matrix measures the extent of artificial details or 'hallucinations' introduced in the SR image. It is a matrix of dimensions HxW. A high value in the hallucination matrix indicates that the SR image has introduced artificial details that are not present in the HR image.","title":"hallucination"},{"location":"docs/API/results_attributes.html#classification","text":"The classification matrix (HxW) is computed by applying a np.argmin function across the aforementioned correctness matrices. This matrix forms the basis for categorizing each pixel into one of the three classes: improvement, omission, and hallucination.","title":"classification"},{"location":"docs/API/results_attributes.html#auxiliary","text":"The auxiliary subset comprises a set of intermediate outputs generated during the execution of the compute method. These results are instrumental in understanding the internal workings and transformations applied during the computation process. There are two auxiliary products: sr_harm , lr_to_hr .","title":"auxiliary"},{"location":"docs/API/results_attributes.html#sr_harm","text":"This is the SR product post-harmonization. The harmonization pipeline is influenced by the harm_apply_spectral and harm_apply_spatial parameters. When both parameters are enabled (set to True, by default), the SR image undergoes a two-step enhancement: first, the reflectance values are corrected via histogram matching with the HR image; subsequently, spatial alignment is performed, aligning the SR image with the HR image based on the method specified in the spatial_method parameter.","title":"sr_harm"},{"location":"docs/API/results_attributes.html#lr_to_hr","text":"This represents the LR image resampled to match the dimensions of the HR image. In the absence of a specific upsample_method set during the setup phase, the lr_to_hr result is achieved using a classic method - bilinear interpolation complemented by an anti-aliasing kernel filter.","title":"lr_to_hr"},{"location":"docs/Metrics/correctness.html","text":"The correctness scores It evaluates the quality of high-frequency information introduced by the SR model. Three correctness scores are implemented in opensr-test : improvement, omission, and hallucination. The correctness scores depend on the distance metric set by the user. Each score reveals different aspects of the SR model performance: Improvement : Low values represent a good match between the SR and HR images. The equation for calculating improvement is: \\(H = d_{im} + d_{om} - 1\\) \\(Improvement = d_{im} + d_{om}*(1 - e^{-\\gamma H})\\) Where: \\(d_{im}\\) is the distance between the SR and HR images. \\(d_{om}\\) is the distance between the SR and LR images. Omission : Low values are related to the inability to represent high-frequency information from the HR and keep similar to the LR image. The equation for calculating omission is: \\(H = d_{im} + d_{om} - 1\\) \\(Omission = d_{om} + d_{im}*(1 - e^{-\\gamma H})\\) Hallucination : Low values shows the areas where the SR model has introduced high-frequency information that is not present in the HR image. The equation for calculating hallucination is: \\(Hallucination = e^{-\\gamma * d_{om} * d_{im}}\\)","title":"Correctness scores"},{"location":"docs/Metrics/correctness.html#_1","text":"","title":""},{"location":"docs/Metrics/correctness.html#the-correctness-scores","text":"It evaluates the quality of high-frequency information introduced by the SR model. Three correctness scores are implemented in opensr-test : improvement, omission, and hallucination. The correctness scores depend on the distance metric set by the user. Each score reveals different aspects of the SR model performance: Improvement : Low values represent a good match between the SR and HR images. The equation for calculating improvement is: \\(H = d_{im} + d_{om} - 1\\) \\(Improvement = d_{im} + d_{om}*(1 - e^{-\\gamma H})\\) Where: \\(d_{im}\\) is the distance between the SR and HR images. \\(d_{om}\\) is the distance between the SR and LR images. Omission : Low values are related to the inability to represent high-frequency information from the HR and keep similar to the LR image. The equation for calculating omission is: \\(H = d_{im} + d_{om} - 1\\) \\(Omission = d_{om} + d_{im}*(1 - e^{-\\gamma H})\\) Hallucination : Low values shows the areas where the SR model has introduced high-frequency information that is not present in the HR image. The equation for calculating hallucination is: \\(Hallucination = e^{-\\gamma * d_{om} * d_{im}}\\)","title":"The correctness scores"},{"location":"docs/Metrics/distance.html","text":"Distance metrics The opensr-test package offers a comprehensive suite of nine different distance metrics designed to assess the consistency, synthesis, and accuracy of super-resolution models. These metrics are structured such that a score of zero represents optimal performance, with higher scores indicating decreasing model effectiveness. The distance metrics available in the opensr-test package include: L1 distance : The L1 distance is the sum of the absolute differences between the two vectors. It is also known as the Manhattan distance. \\(L1(y, \u0177) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \u0177_i|\\) L2 distance : The L2 distance is the square root of the sum of the squared differences between the two vectors. It is also known as the Euclidean distance. \\(L2(y, \u0177) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \u0177_i)^2\\) Spectral angle distance : The spectral angle distance is the angle between two vectors. The angle is estimated in degrees. \\(SAD(\\vec{y}, \\vec{\\hat{y}}) = \\arccos\\left(\\frac{\\vec{y} \\cdot \\vec{\\hat{y}}}{\\|\\vec{y}\\| \\|\\vec{\\hat{y}}\\|}\\right)\\) Percentage Bias : The Percentage Bias (PBIAS) measures the average tendency of the super-resolved values to be larger or smaller than their observed counterparts. This metric help us to understand whether a model is changing the norm of the original reflectance values. The equation for calculating Percentage Bias is: \\(PBIAS = \\left( \\frac{\\sum_{i=1}^{n} (O_i - S_i)}{\\sum_{i=1}^{n} O_i} \\right)\\) Inverted Peak Signal-to-Noise Ratio : The Inverted Peak Signal-to-Noise Ratio (IPSNR) is the inverse of the Peak Signal-to-Noise Ratio (PSNR). The equation for calculating IPSNR is: $\\text{PSNR} = \\text{MAX}_I^2 \\cdot 10^{-\\frac{\\text{MSE}}{10}} $ Where \\(\\text{MAX}_I\\) is set to 1, and \\(\\text{MSE}\\) is the Mean Squared Error. Kullback-Leibler divergence : The Kullback-Leibler divergence (KLD) is a measure of how one probability distribution is different from a second, reference probability distribution. The equation for calculating KLD is: \\(KL(P || Q) = \\sum_{x} P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right)\\) LPIPS : The Learned Perceptual Image Patch Similarity (LPIPS) metric is a perceptual metric that aims to quantify the perceptual similarity between two images. The LPIPS metric is based on a deep neural network that was trained to predict perceptual similarity scores. The reported metric is the average LPIPS score between the LR and SR images. \\(\\text{LPIPS} = \\sum_{l=1}^{L} w_l \\cdot \\frac{1}{H_lW_l} \\sum_{h=1}^{H_l} \\sum_{w=1}^{W_l} \\| \\phi_l(I_1)_{h,w} - \\phi_l(I_2)_{h,w} \\|_2^2\\) In this equation: \\(LPIPS\\) is the Learned Perceptual Image Patch Similarity score. \\(L\\) denotes the number of layers in a deep neural network used for comparison. \\(w_l\\) represents the weight of the \\(l-th\\) layer in the network. \\(\\phi_l(l)_{I}\\) is the feature map of image \\(I\\) at layer \\(l\\) . \\(H_l\\) and \\(W_l\\) are the height and width of the feature map at layer \\(l\\) , respectively. \\(I_1\\) and \\(I_2\\) are the two images being compared. The summations across \\(h\\) and \\(w\\) are over the spatial dimensions of the feature maps. CLIP CLIP measures the distance (L1) in image embedding space, see CLIP model. Unlike LPIPS, with CLIPscore we can focus mainly in the contextual and semantic integrity of the super-resolved images. We use the RemoteCLIP pretrained model. Please cite the following paper if you use this metric: @article { liu2024remoteclip , title = {Remoteclip: A vision language foundation model for remote sensing} , author = {Liu, Fan and Chen, Delong and Guan, Zhangqingyun and Zhou, Xiaocong and Zhu, Jiale and Ye, Qiaolin and Fu, Liyong and Zhou, Jun} , journal = {IEEE Transactions on Geoscience and Remote Sensing} , year = {2024} , publisher = {IEEE} } MTF The Modulation Transfer Function (MTF) is a metric that measures the ability of an imaging system to reproduce the spatial frequencies of an object. The MTF is calculated as the ratio of the SR image to the HR image in the frequency domain: \\(MTF = \\frac{|\\text{FFT}(HR) - \\text{FFT}(SR)|}{|\\text{FFT}(HR)|}\\) Where \\(\\text{FFT}\\) is the Fast Fourier Transform after the Sentinel-2 Nyquist frequency.","title":"Distance metrics"},{"location":"docs/Metrics/distance.html#_1","text":"","title":""},{"location":"docs/Metrics/distance.html#distance-metrics","text":"The opensr-test package offers a comprehensive suite of nine different distance metrics designed to assess the consistency, synthesis, and accuracy of super-resolution models. These metrics are structured such that a score of zero represents optimal performance, with higher scores indicating decreasing model effectiveness. The distance metrics available in the opensr-test package include: L1 distance : The L1 distance is the sum of the absolute differences between the two vectors. It is also known as the Manhattan distance. \\(L1(y, \u0177) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \u0177_i|\\) L2 distance : The L2 distance is the square root of the sum of the squared differences between the two vectors. It is also known as the Euclidean distance. \\(L2(y, \u0177) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \u0177_i)^2\\) Spectral angle distance : The spectral angle distance is the angle between two vectors. The angle is estimated in degrees. \\(SAD(\\vec{y}, \\vec{\\hat{y}}) = \\arccos\\left(\\frac{\\vec{y} \\cdot \\vec{\\hat{y}}}{\\|\\vec{y}\\| \\|\\vec{\\hat{y}}\\|}\\right)\\) Percentage Bias : The Percentage Bias (PBIAS) measures the average tendency of the super-resolved values to be larger or smaller than their observed counterparts. This metric help us to understand whether a model is changing the norm of the original reflectance values. The equation for calculating Percentage Bias is: \\(PBIAS = \\left( \\frac{\\sum_{i=1}^{n} (O_i - S_i)}{\\sum_{i=1}^{n} O_i} \\right)\\) Inverted Peak Signal-to-Noise Ratio : The Inverted Peak Signal-to-Noise Ratio (IPSNR) is the inverse of the Peak Signal-to-Noise Ratio (PSNR). The equation for calculating IPSNR is: $\\text{PSNR} = \\text{MAX}_I^2 \\cdot 10^{-\\frac{\\text{MSE}}{10}} $ Where \\(\\text{MAX}_I\\) is set to 1, and \\(\\text{MSE}\\) is the Mean Squared Error. Kullback-Leibler divergence : The Kullback-Leibler divergence (KLD) is a measure of how one probability distribution is different from a second, reference probability distribution. The equation for calculating KLD is: \\(KL(P || Q) = \\sum_{x} P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right)\\) LPIPS : The Learned Perceptual Image Patch Similarity (LPIPS) metric is a perceptual metric that aims to quantify the perceptual similarity between two images. The LPIPS metric is based on a deep neural network that was trained to predict perceptual similarity scores. The reported metric is the average LPIPS score between the LR and SR images. \\(\\text{LPIPS} = \\sum_{l=1}^{L} w_l \\cdot \\frac{1}{H_lW_l} \\sum_{h=1}^{H_l} \\sum_{w=1}^{W_l} \\| \\phi_l(I_1)_{h,w} - \\phi_l(I_2)_{h,w} \\|_2^2\\) In this equation: \\(LPIPS\\) is the Learned Perceptual Image Patch Similarity score. \\(L\\) denotes the number of layers in a deep neural network used for comparison. \\(w_l\\) represents the weight of the \\(l-th\\) layer in the network. \\(\\phi_l(l)_{I}\\) is the feature map of image \\(I\\) at layer \\(l\\) . \\(H_l\\) and \\(W_l\\) are the height and width of the feature map at layer \\(l\\) , respectively. \\(I_1\\) and \\(I_2\\) are the two images being compared. The summations across \\(h\\) and \\(w\\) are over the spatial dimensions of the feature maps. CLIP CLIP measures the distance (L1) in image embedding space, see CLIP model. Unlike LPIPS, with CLIPscore we can focus mainly in the contextual and semantic integrity of the super-resolved images. We use the RemoteCLIP pretrained model. Please cite the following paper if you use this metric: @article { liu2024remoteclip , title = {Remoteclip: A vision language foundation model for remote sensing} , author = {Liu, Fan and Chen, Delong and Guan, Zhangqingyun and Zhou, Xiaocong and Zhu, Jiale and Ye, Qiaolin and Fu, Liyong and Zhou, Jun} , journal = {IEEE Transactions on Geoscience and Remote Sensing} , year = {2024} , publisher = {IEEE} } MTF The Modulation Transfer Function (MTF) is a metric that measures the ability of an imaging system to reproduce the spatial frequencies of an object. The MTF is calculated as the ratio of the SR image to the HR image in the frequency domain: \\(MTF = \\frac{|\\text{FFT}(HR) - \\text{FFT}(SR)|}{|\\text{FFT}(HR)|}\\) Where \\(\\text{FFT}\\) is the Fast Fourier Transform after the Sentinel-2 Nyquist frequency.","title":"Distance metrics"}]}