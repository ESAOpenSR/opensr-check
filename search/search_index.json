{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"A comprehensive benchmark for real-world Sentinel-2 imagery super-resolution GitHub : https://github.com/ESAOpenSR/opensr-test Documentation : https://esaopensr.github.io/opensr-test PyPI : https://pypi.org/project/opensr-test/ Paper : Coming soon! Overview In remote sensing, Image Super-Resolution (ISR) goal is to improve the ground sampling distance. However, two problems are common in the literature. First, most models are tested on synthetic data , raising doubts about their real-world applicability and performance. Second, traditional evaluation metrics such as PSNR, LPIPS, and SSIM are not designed for assessing ISR performance. These metrics fall short, especially in conditions involving changes in luminance or spatial misalignments - scenarios that are frequently encountered in remote sensing imagery. To address these challenges, 'opensr-test' provides a fair approach for ISR benchmark. We provide three datasets that were carefully crafted to minimize spatial and spectral misalignment. Besides, 'opensr-test' precisely assesses ISR algorithm performance across three independent metrics groups that measure consistency , synthesis , and correctness . How to use The example below shows how to use opensr-test to benchmark your SR model. import torch import opensr_test lr = torch . rand ( 4 , 64 , 64 ) hr = torch . rand ( 4 , 256 , 256 ) sr = torch . rand ( 4 , 256 , 256 ) metrics = opensr_test . Metrics () metrics . compute ( lr = lr , sr = sr , hr = hr ) Benchmark Benchmark comparison of SR models. Downward arrows (\u2193) denote metrics in which lower values are preferable, and upward arrows (\u2191) indicate metrics in which higher values reflect better performance. Consistency Synthesis Correctness reflectance \u2193 spectral \u2193 spatial \u2193 high-frequency \u2191 ha \u2193 om \u2193 im \u2191 NAIP SuperImage 0.008 7.286 0.131 0.003 0.117 0.784 0.098 SR4RS 0.016 3.471 1.156 0.010 0.869 0.077 0.054 diffusers 0.463 12.437 2.88 0.013 0.905 0.055 0.040 SPOT SuperImage 0.009 3.512 0.062 0.006 0.160 0.794 0.046 SR4RS 0.039 3.232 1.151 0.023 0.834 0.115 0.051 diffusers 0.417 11.730 0.817 0.014 0.686 0.251 0.063 VEN\u00b5S SuperImage 0.009 8.687 0.099 0.003 0.403 0.380 0.217 SR4RS 0.014 3.394 1.122 0.012 0.971 0.017 0.012 diffusers 0.467 13.303 0.806 0.009 0.933 0.043 0.024 Installation Install the latest version from PyPI: pip install opensr-test Upgrade opensr-test by running: pip install -U opensr-test Install the latest dev version from GitHub by running: pip install git+https://github.com/ESAOpenSR/opensr-test Examples The following examples show how to use opensr-test to benchmark your SR model. Use opensr-test with TensorFlow model Use opensr-test with PyTorch model Use opensr-test with a diffuser model Visualizations The opensr-test package provides a set of visualizations to help you understand the performance of your SR model. import torch import opensr_test import matplotlib.pyplot as plt from super_image import HanModel # Define the SR model srmodel = HanModel . from_pretrained ( 'eugenesiow/han' , scale = 4 ) # Load the data lr , hr , landuse , parameters = opensr_test . load ( \"spot\" ) . values () # Define the benchmark experiment metrics = opensr_test . Metrics () # Define the image to be tested idx = 0 lr_img = torch . from_numpy ( lr [ idx , 0 : 3 ]) hr_img = torch . from_numpy ( hr [ idx , 0 : 3 ]) sr_img = srmodel ( lr_img [ None ]) . squeeze () . detach () # Compute the metrics metrics . compute ( lr = lr_img , sr = sr_img , hr = hr_img , stability_threshold = parameters . stability_threshold [ idx ], im_score = parameters . correctness_params [ 0 ], om_score = parameters . correctness_params [ 1 ], ha_score = parameters . correctness_params [ 2 ] ) Now, we can visualize the results using the opensr_test.visualize module. fDisplay the triplets LR, SR and HR images: metrics . plot_triplets () Display the quadruplets LR, SR, HR and landuse images: metrics . plot_quadruplets () Display the matching points between the LR and SR images: metrics . plot_spatial_matches () Display a summary of all the metrics: metrics . plot_summary () Display the correctness of the SR image: metrics . plot_tc () Deeper understanding Explore the API section for more details about personalizing your benchmark experiments. Citation If you use opensr-test in your research, please cite our paper: Coming soon! Acknowledgements This work was make with the support of the European Space Agency (ESA) under the project \u201cExplainable AI: application to trustworthy super-resolution (OpenSR)\u201d. Cesar Aybar acknowledges support by the National Council of Science, Technology, and Technological Innovation (CONCYTEC, Peru) through the \u201cPROYECTOS DE INVESTIGACI\u00d3N B\u00c1SICA \u2013 2023-01\u201d program with contract number PE501083135-2023-PROCIENCIA. Luis G\u00f3mez-Chova acknowledges support from the Spanish Ministry of Science and Innovation (project PID2019-109026RB-I00 funded by MCIN/AEI/10.13039/501100011033).","title":"Index"},{"location":"index.html#_1","text":"","title":""},{"location":"index.html#overview","text":"In remote sensing, Image Super-Resolution (ISR) goal is to improve the ground sampling distance. However, two problems are common in the literature. First, most models are tested on synthetic data , raising doubts about their real-world applicability and performance. Second, traditional evaluation metrics such as PSNR, LPIPS, and SSIM are not designed for assessing ISR performance. These metrics fall short, especially in conditions involving changes in luminance or spatial misalignments - scenarios that are frequently encountered in remote sensing imagery. To address these challenges, 'opensr-test' provides a fair approach for ISR benchmark. We provide three datasets that were carefully crafted to minimize spatial and spectral misalignment. Besides, 'opensr-test' precisely assesses ISR algorithm performance across three independent metrics groups that measure consistency , synthesis , and correctness .","title":"Overview"},{"location":"index.html#how-to-use","text":"The example below shows how to use opensr-test to benchmark your SR model. import torch import opensr_test lr = torch . rand ( 4 , 64 , 64 ) hr = torch . rand ( 4 , 256 , 256 ) sr = torch . rand ( 4 , 256 , 256 ) metrics = opensr_test . Metrics () metrics . compute ( lr = lr , sr = sr , hr = hr )","title":"How to use"},{"location":"index.html#benchmark","text":"Benchmark comparison of SR models. Downward arrows (\u2193) denote metrics in which lower values are preferable, and upward arrows (\u2191) indicate metrics in which higher values reflect better performance. Consistency Synthesis Correctness reflectance \u2193 spectral \u2193 spatial \u2193 high-frequency \u2191 ha \u2193 om \u2193 im \u2191 NAIP SuperImage 0.008 7.286 0.131 0.003 0.117 0.784 0.098 SR4RS 0.016 3.471 1.156 0.010 0.869 0.077 0.054 diffusers 0.463 12.437 2.88 0.013 0.905 0.055 0.040 SPOT SuperImage 0.009 3.512 0.062 0.006 0.160 0.794 0.046 SR4RS 0.039 3.232 1.151 0.023 0.834 0.115 0.051 diffusers 0.417 11.730 0.817 0.014 0.686 0.251 0.063 VEN\u00b5S SuperImage 0.009 8.687 0.099 0.003 0.403 0.380 0.217 SR4RS 0.014 3.394 1.122 0.012 0.971 0.017 0.012 diffusers 0.467 13.303 0.806 0.009 0.933 0.043 0.024","title":"Benchmark"},{"location":"index.html#installation","text":"Install the latest version from PyPI: pip install opensr-test Upgrade opensr-test by running: pip install -U opensr-test Install the latest dev version from GitHub by running: pip install git+https://github.com/ESAOpenSR/opensr-test","title":"Installation"},{"location":"index.html#examples","text":"The following examples show how to use opensr-test to benchmark your SR model. Use opensr-test with TensorFlow model Use opensr-test with PyTorch model Use opensr-test with a diffuser model","title":"Examples"},{"location":"index.html#visualizations","text":"The opensr-test package provides a set of visualizations to help you understand the performance of your SR model. import torch import opensr_test import matplotlib.pyplot as plt from super_image import HanModel # Define the SR model srmodel = HanModel . from_pretrained ( 'eugenesiow/han' , scale = 4 ) # Load the data lr , hr , landuse , parameters = opensr_test . load ( \"spot\" ) . values () # Define the benchmark experiment metrics = opensr_test . Metrics () # Define the image to be tested idx = 0 lr_img = torch . from_numpy ( lr [ idx , 0 : 3 ]) hr_img = torch . from_numpy ( hr [ idx , 0 : 3 ]) sr_img = srmodel ( lr_img [ None ]) . squeeze () . detach () # Compute the metrics metrics . compute ( lr = lr_img , sr = sr_img , hr = hr_img , stability_threshold = parameters . stability_threshold [ idx ], im_score = parameters . correctness_params [ 0 ], om_score = parameters . correctness_params [ 1 ], ha_score = parameters . correctness_params [ 2 ] ) Now, we can visualize the results using the opensr_test.visualize module. fDisplay the triplets LR, SR and HR images: metrics . plot_triplets () Display the quadruplets LR, SR, HR and landuse images: metrics . plot_quadruplets () Display the matching points between the LR and SR images: metrics . plot_spatial_matches () Display a summary of all the metrics: metrics . plot_summary () Display the correctness of the SR image: metrics . plot_tc ()","title":"Visualizations"},{"location":"index.html#deeper-understanding","text":"Explore the API section for more details about personalizing your benchmark experiments.","title":"Deeper understanding"},{"location":"index.html#citation","text":"If you use opensr-test in your research, please cite our paper: Coming soon!","title":"Citation"},{"location":"index.html#acknowledgements","text":"This work was make with the support of the European Space Agency (ESA) under the project \u201cExplainable AI: application to trustworthy super-resolution (OpenSR)\u201d. Cesar Aybar acknowledges support by the National Council of Science, Technology, and Technological Innovation (CONCYTEC, Peru) through the \u201cPROYECTOS DE INVESTIGACI\u00d3N B\u00c1SICA \u2013 2023-01\u201d program with contract number PE501083135-2023-PROCIENCIA. Luis G\u00f3mez-Chova acknowledges support from the Spanish Ministry of Science and Innovation (project PID2019-109026RB-I00 funded by MCIN/AEI/10.13039/501100011033).","title":"Acknowledgements"},{"location":"NEWS.html","text":"0.2.0 Several bugs have been fixed. 0.1.2 Bug in visualization triplets and quads fixed. 0.1.0 Initial release First version of the package now available on pypi.","title":"0.2.0"},{"location":"NEWS.html#020","text":"Several bugs have been fixed.","title":"0.2.0"},{"location":"NEWS.html#012","text":"Bug in visualization triplets and quads fixed.","title":"0.1.2"},{"location":"NEWS.html#010-initial-release","text":"First version of the package now available on pypi.","title":"0.1.0 Initial release"},{"location":"docs/CHANGELOG.html","text":"Changelog All notable changes to the opensr-test project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Added For new features. Changed For changes in existing functionality. Deprecated For soon-to-be removed features. Removed For now removed features. Fixed For any bug fixes. Security In case of vulnerabilities. Unreleased Unreleased changes here. 0.1.0 - 2023-12-20 Added First release of the opensr-test package.","title":"Changelog"},{"location":"docs/CHANGELOG.html#changelog","text":"All notable changes to the opensr-test project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"docs/CHANGELOG.html#added","text":"For new features.","title":"Added"},{"location":"docs/CHANGELOG.html#changed","text":"For changes in existing functionality.","title":"Changed"},{"location":"docs/CHANGELOG.html#deprecated","text":"For soon-to-be removed features.","title":"Deprecated"},{"location":"docs/CHANGELOG.html#removed","text":"For now removed features.","title":"Removed"},{"location":"docs/CHANGELOG.html#fixed","text":"For any bug fixes.","title":"Fixed"},{"location":"docs/CHANGELOG.html#security","text":"In case of vulnerabilities.","title":"Security"},{"location":"docs/CHANGELOG.html#unreleased","text":"Unreleased changes here.","title":"Unreleased"},{"location":"docs/CHANGELOG.html#010-2023-12-20","text":"","title":"0.1.0 - 2023-12-20"},{"location":"docs/CHANGELOG.html#added_1","text":"First release of the opensr-test package.","title":"Added"},{"location":"docs/CONTRIBUTING.html","text":"Contributing to opensr-test We love your input! We want to make contributing to this project as easy and transparent as possible, whether it's: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Becoming a maintainer We Develop with Github We use GitHub to host code, to track issues and feature requests, as well as accept pull requests. Pull requests are the best way to propose changes to the codebase. We actively welcome your pull requests: Fork the repo and create your branch from main . If you've added code that should be tested, add tests. If you've changed APIs, update the documentation. Ensure the test suite passes. Make sure your code lints. Issue that pull request! Any contributions you make will be under the MIT Software License In short, when you submit code changes, your submissions are understood to be under the same MIT License that covers the project. Feel free to contact the maintainers if that's a concern. Report bugs using Github's issues We use GitHub issues to track public bugs. Report a bug by opening a new issue ; it's that easy! Write bug reports with detail, background, and sample code Great Bug Reports tend to have: A quick summary and/or background Steps to reproduce Be specific! Give sample code if you can. What you expected would happen What actually happens Notes (possibly including why you think this might be happening, or stuff you tried that didn't work) People love thorough bug reports. Use a Consistent Coding Style 4 spaces for indentation rather than tabs Follow PEP8 where possible. If possible, use Black to format your code. Add comments to your code where necessary. In Python code, use docstrings. License By contributing, you agree that your contributions will be licensed under its MIT License.","title":"Contributing"},{"location":"docs/CONTRIBUTING.html#contributing-to-opensr-test","text":"We love your input! We want to make contributing to this project as easy and transparent as possible, whether it's: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Becoming a maintainer","title":"Contributing to opensr-test"},{"location":"docs/CONTRIBUTING.html#we-develop-with-github","text":"We use GitHub to host code, to track issues and feature requests, as well as accept pull requests. Pull requests are the best way to propose changes to the codebase. We actively welcome your pull requests: Fork the repo and create your branch from main . If you've added code that should be tested, add tests. If you've changed APIs, update the documentation. Ensure the test suite passes. Make sure your code lints. Issue that pull request!","title":"We Develop with Github"},{"location":"docs/CONTRIBUTING.html#any-contributions-you-make-will-be-under-the-mit-software-license","text":"In short, when you submit code changes, your submissions are understood to be under the same MIT License that covers the project. Feel free to contact the maintainers if that's a concern.","title":"Any contributions you make will be under the MIT Software License"},{"location":"docs/CONTRIBUTING.html#report-bugs-using-githubs-issues","text":"We use GitHub issues to track public bugs. Report a bug by opening a new issue ; it's that easy!","title":"Report bugs using Github's issues"},{"location":"docs/CONTRIBUTING.html#write-bug-reports-with-detail-background-and-sample-code","text":"Great Bug Reports tend to have: A quick summary and/or background Steps to reproduce Be specific! Give sample code if you can. What you expected would happen What actually happens Notes (possibly including why you think this might be happening, or stuff you tried that didn't work) People love thorough bug reports.","title":"Write bug reports with detail, background, and sample code"},{"location":"docs/CONTRIBUTING.html#use-a-consistent-coding-style","text":"4 spaces for indentation rather than tabs Follow PEP8 where possible. If possible, use Black to format your code. Add comments to your code where necessary. In Python code, use docstrings.","title":"Use a Consistent Coding Style"},{"location":"docs/CONTRIBUTING.html#license","text":"By contributing, you agree that your contributions will be licensed under its MIT License.","title":"License"},{"location":"docs/Code_of_Conduct.html","text":"Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [cesar.aybar@uv.es]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct/code_of_conduct.md For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"docs/Code_of_Conduct.html#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"docs/Code_of_Conduct.html#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"docs/Code_of_Conduct.html#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"docs/Code_of_Conduct.html#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"docs/Code_of_Conduct.html#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"docs/Code_of_Conduct.html#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [cesar.aybar@uv.es]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"docs/Code_of_Conduct.html#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct/code_of_conduct.md For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"docs/LICENSE.html","text":"The MIT License (MIT) Copyright (c) 2023 OpenSR team Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"docs/API/compute_method.html","text":"The compute Method The compute method calculate the suite of metrics in the opensr-test framework. This method includes four optional parameters, all of them related to the calculation of correctness metrics ( Metrics section section). These parameters are: stability_threshold (default=0.01): This threshold help for discerning pixels with significant distance differences within the triple image space (LR, SR, and HR). Setting this parameter helps in isolating pixels where the model has introduced notable high-frequency details, thereby focusing the analysis on areas of potential enhancement. By default, this parameter is set to 0.01, which is optimized for balanced assessment. However, this parameter can be adjusted to each image. Based on the expert judgment of three remote sensing specialists, the optimal value of this parameter has been determined for each image within the datasets of NAIP, SPOT, and Venus, allowing for a more tailored and precise evaluation for each specific dataset. im_score (default=0.8): This parameter is critical to defining the 'improvement space'. It acts as a modulator determining whether a pixel/patch is considered 'improved'. The default value is set to 0.8. This value have been determined by the perceptual evaluation of three remote sensing experts. om_score (default=0.8): Similarly, the 'omission space' is determined by this parameter. The default of 0.8. This value have been determined by the perceptual evaluation of three remote sensing experts. ha_score (default=0.4): This parameter delineates the 'hallucination space', evaluating whether a pixel is regarded as a hallucination. The default setting of 0.4 have been determined by the perceptual evaluation of three remote sensing experts. Below is an example that demonstrates the usage of the compute method with the aforementioned parameters: # Import necessary libraries import torch import opensr_test import matplotlib.pyplot as plt # Generate sample LR, HR, and SR images lr = torch . rand ( 4 , 32 , 32 ) # Low Resolution image hr = torch . rand ( 4 , 256 , 256 ) # High Resolution image sr = torch . rand ( 4 , 256 , 256 ) # Super Resolution image # Initialize the Metrics object metrics = opensr_test . Metrics () # Compute the metrics with specified parameters metrics . compute ( lr = lr , sr = sr , hr = hr , stability_threshold = 0.01 , im_score = 0.8 , om_score = 0.8 , ha_score = 0.4 , )","title":"Compute method"},{"location":"docs/API/compute_method.html#_1","text":"","title":""},{"location":"docs/API/compute_method.html#the-compute-method","text":"The compute method calculate the suite of metrics in the opensr-test framework. This method includes four optional parameters, all of them related to the calculation of correctness metrics ( Metrics section section). These parameters are: stability_threshold (default=0.01): This threshold help for discerning pixels with significant distance differences within the triple image space (LR, SR, and HR). Setting this parameter helps in isolating pixels where the model has introduced notable high-frequency details, thereby focusing the analysis on areas of potential enhancement. By default, this parameter is set to 0.01, which is optimized for balanced assessment. However, this parameter can be adjusted to each image. Based on the expert judgment of three remote sensing specialists, the optimal value of this parameter has been determined for each image within the datasets of NAIP, SPOT, and Venus, allowing for a more tailored and precise evaluation for each specific dataset. im_score (default=0.8): This parameter is critical to defining the 'improvement space'. It acts as a modulator determining whether a pixel/patch is considered 'improved'. The default value is set to 0.8. This value have been determined by the perceptual evaluation of three remote sensing experts. om_score (default=0.8): Similarly, the 'omission space' is determined by this parameter. The default of 0.8. This value have been determined by the perceptual evaluation of three remote sensing experts. ha_score (default=0.4): This parameter delineates the 'hallucination space', evaluating whether a pixel is regarded as a hallucination. The default setting of 0.4 have been determined by the perceptual evaluation of three remote sensing experts. Below is an example that demonstrates the usage of the compute method with the aforementioned parameters: # Import necessary libraries import torch import opensr_test import matplotlib.pyplot as plt # Generate sample LR, HR, and SR images lr = torch . rand ( 4 , 32 , 32 ) # Low Resolution image hr = torch . rand ( 4 , 256 , 256 ) # High Resolution image sr = torch . rand ( 4 , 256 , 256 ) # Super Resolution image # Initialize the Metrics object metrics = opensr_test . Metrics () # Compute the metrics with specified parameters metrics . compute ( lr = lr , sr = sr , hr = hr , stability_threshold = 0.01 , im_score = 0.8 , om_score = 0.8 , ha_score = 0.4 , )","title":"The compute Method"},{"location":"docs/API/config_pydantic.html","text":"Config The Config class is a pydantic.BaseModel for defining model parameters, ensuring type safety and ease of configuration. The parameters are categorized into four groups, each catering to different aspects of the super-resolution (SR) evaluation process: Spectral Parameters: These parameters are specific to spectral analysis and alignment: reflectance_method: The default method for reflectance estimation is \"l1\", with \"kl\", \"l1\", \"l2\", and \"pbias\" as alternatives. spectral_method: For spectral alignment, \"sad\" is the default method. Spatial Parameters: These parameters fine-tune the spatial feature extraction and matching process: spatial_features: The default feature extractor is \"superpoint\", with \"disk\" also available. spatial_matcher: Sets the matcher for feature points, defaulting to \"lightglue\". spatial_max_num_keypoints: Caps the number of keypoints for feature matching at 1000 by default. spatial_threshold_distance: The maximum permissible distance between keypoints for a match, set to 5 pixels. spatial_threshold_npoints: The minimum required keypoints for alignment, set to 5. Spatial alignment is skipped if keypoints fall below this threshold. Interpolation Parameters: upsample_method : The default method for downscaling is \"classic\", with \"naip\", \"spot\", and \"venus\" as alternatives. downsample_method : The default method for upscaling is \"bicubic\" with antialias. Global Parameters: These parameters control the overall behavior of the SR assessment: device: Selects the computation device for inference, defaulting to cpu . cuda is also supported for GPU acceleration. distance_method: Specifies the distance metric between SR, LR, and HR images. Defaults to \"l1\", with support for other metrics like kl, l2, pbias, rmse, ipsnr, sad, clip and lpips. See section Metrics for more details. agg_method: Determines the granularity for distance metric application \u2014 pixel-wise by default (\"pixel\"), with \"patch\" and \"image\" level aggregations options. patch_size: Relevant when agg_method is \"patch\", this defines the patch size for distance metric computation. border_mask: Excludes image borders in metric calculations, ignoring the outer 16 pixels by default. rgb_bands: Necessary for certain metrics like LPIPS and plot generation, assuming a default order of Red-Green-Blue ([0, 1, 2]). harm_apply_spectral: Applies histogram matching before distance metric calculation, enabled by default. harm_apply_spatial: Activates spatial alignment before distance metric calculation, enabled by default. Below is an illustrative example of how to instantiate opensr-test with user-defined parameters: import torch import opensr_test # Define the Low Resolution (LR), High Resolution (HR), and Super-Resolved (SR) images. lr = torch . rand ( 4 , 64 , 64 ) hr = torch . rand ( 4 , 256 , 256 ) sr = torch . rand ( 4 , 256 , 256 ) # Initialize the Metrics object with custom parameters. config = opensr_test . Config ( device = \"cpu\" , spatial_features = \"disk\" , spatial_max_num_keypoints = 400 , harm_apply_spectral = False , ) metrics = opensr_test . Metrics ( config ) # Compute the metrics based on the provided images. metrics . compute ( lr = lr , sr = sr , hr = hr )","title":"Config"},{"location":"docs/API/config_pydantic.html#_1","text":"","title":""},{"location":"docs/API/config_pydantic.html#config","text":"The Config class is a pydantic.BaseModel for defining model parameters, ensuring type safety and ease of configuration. The parameters are categorized into four groups, each catering to different aspects of the super-resolution (SR) evaluation process:","title":"Config"},{"location":"docs/API/config_pydantic.html#spectral-parameters","text":"These parameters are specific to spectral analysis and alignment: reflectance_method: The default method for reflectance estimation is \"l1\", with \"kl\", \"l1\", \"l2\", and \"pbias\" as alternatives. spectral_method: For spectral alignment, \"sad\" is the default method.","title":"Spectral Parameters:"},{"location":"docs/API/config_pydantic.html#spatial-parameters","text":"These parameters fine-tune the spatial feature extraction and matching process: spatial_features: The default feature extractor is \"superpoint\", with \"disk\" also available. spatial_matcher: Sets the matcher for feature points, defaulting to \"lightglue\". spatial_max_num_keypoints: Caps the number of keypoints for feature matching at 1000 by default. spatial_threshold_distance: The maximum permissible distance between keypoints for a match, set to 5 pixels. spatial_threshold_npoints: The minimum required keypoints for alignment, set to 5. Spatial alignment is skipped if keypoints fall below this threshold.","title":"Spatial Parameters:"},{"location":"docs/API/config_pydantic.html#interpolation-parameters","text":"upsample_method : The default method for downscaling is \"classic\", with \"naip\", \"spot\", and \"venus\" as alternatives. downsample_method : The default method for upscaling is \"bicubic\" with antialias.","title":"Interpolation Parameters:"},{"location":"docs/API/config_pydantic.html#global-parameters","text":"These parameters control the overall behavior of the SR assessment: device: Selects the computation device for inference, defaulting to cpu . cuda is also supported for GPU acceleration. distance_method: Specifies the distance metric between SR, LR, and HR images. Defaults to \"l1\", with support for other metrics like kl, l2, pbias, rmse, ipsnr, sad, clip and lpips. See section Metrics for more details. agg_method: Determines the granularity for distance metric application \u2014 pixel-wise by default (\"pixel\"), with \"patch\" and \"image\" level aggregations options. patch_size: Relevant when agg_method is \"patch\", this defines the patch size for distance metric computation. border_mask: Excludes image borders in metric calculations, ignoring the outer 16 pixels by default. rgb_bands: Necessary for certain metrics like LPIPS and plot generation, assuming a default order of Red-Green-Blue ([0, 1, 2]). harm_apply_spectral: Applies histogram matching before distance metric calculation, enabled by default. harm_apply_spatial: Activates spatial alignment before distance metric calculation, enabled by default. Below is an illustrative example of how to instantiate opensr-test with user-defined parameters: import torch import opensr_test # Define the Low Resolution (LR), High Resolution (HR), and Super-Resolved (SR) images. lr = torch . rand ( 4 , 64 , 64 ) hr = torch . rand ( 4 , 256 , 256 ) sr = torch . rand ( 4 , 256 , 256 ) # Initialize the Metrics object with custom parameters. config = opensr_test . Config ( device = \"cpu\" , spatial_features = \"disk\" , spatial_max_num_keypoints = 400 , harm_apply_spectral = False , ) metrics = opensr_test . Metrics ( config ) # Compute the metrics based on the provided images. metrics . compute ( lr = lr , sr = sr , hr = hr )","title":"Global Parameters:"},{"location":"docs/API/results_attributes.html","text":"The results Attribute The results attribute encapsulates all the metrics calculated at the agg_method level and the intermediate results produced by the opensr-test test. It organizes the output into four principal categories: consistency , distance , correctness , and auxiliary . The following example illustrates how to retrieve the outputs from the results attribute: # Import necessary libraries import torch import opensr_test # Generate sample LR, HR, and SR images lr = torch . rand ( 4 , 64 , 64 ) # Low Resolution image hr = torch . rand ( 4 , 256 , 256 ) # High Resolution image sr = torch . rand ( 4 , 256 , 256 ) # Super Resolution image # Initialize the Metrics object metrics = opensr_test . Metrics () # Compute the metrics metrics . compute ( lr = lr , sr = sr , hr = hr ) # Accessing Consistency Metrics metrics . results . consistency # Accessing Distance Metrics post SR Harmonization metrics . results . distance ## Accessing Correctness Metrics metrics . results . correctness # Accessing Intermediate Results metrics . results . auxiliary Except the auxiliary field, the spatial resolution of all the results is determined by the agg_method parameter. For instance, if agg_method is set to \"patch\" with a patch_size of 32, then each metric in the results attribute will contain values aggregated over patches of size H/32 x W/32. consistency The consistency metrics within opensr-test play a crucial role in evaluating the harmony between LR and SR images prior to SR harmonization (SRharm). These metrics are calculated after resampling the SR images to match the dimensions of the LR (SRdown). There are three key metrics in this category: reflectance , spectral , and spatial . reflectance This metric evaluates how well the SR image reflects the norm values of the LR image. The calculation of reflectance consistency utilizes the method defined by the reflectance_method parameter. spectral This metric assesses the similarity in spectral characteristics between the LR and SR images. The computation of spectral consistency leverages the angle distance specified in the spectral_method parameter. This allows for a detailed comparison of the spectral profiles of the images, ensuring that the SR image preserves the original spectral properties of the LR image. spatial The spatial consistency metric is computed by calculating the difference between the matching points identified in the LR and HR images. This process evaluates the spatial alignment and structural integrity of the SR image compared to the LR image. If the agg_method parameter is set to pixel or patch, the grid is calculated applying a simple kernel interpolation method. distance distance metrics in opensr-test permit measure how far the SR image is from the LR and HR images. These metrics are computed post-harmonization (SRharm) to reduce the potential spatial and spectral bias introduced by the SR model. There are three distance metrics: sr_lr , sr_hr , and hr_lr . sr_to_lr (SRharm - LR Distance) This metric quantifies the distance between SRharm and the LR image. It serves as an indicator of how close the SR image is to the LR image. The LR is upsampled (LRup) to the dimensions of the HR image using the method defined by the upsample_method parameter. We strongly advise opting for a method that does not require parameter tuning, such as bilinear interpolation or other similar techniques. This is crucial to prevent the introduction of hallucination artifacts, which can significantly bias the experimental results. sr_to_hr (SRharm - HR Distance) This metric measures the distance between the harmonized SR image (SRharm) and the HR image. It is essential for measure if the high-frequency details introduced by the SR model are consistent with the HR image (improvement) or if they are artificial (hallucination). lr_to_hr (HR - LR Distance) This metric calculates the distance between the HR and LR images. Although it doesn't directly involve the SR image, it offers a baseline understanding of the initial discrepancies between the HR and LR images, which can be useful for context and comparison. correctness The correctness metrics in opensr-test are crucial assessments conducted after the SR harmonization process and the evaluation of the triple distance. These metrics encompass four categories: improvement , omission , hallucination , and classification . All the correctness metrics are designed such that values closer to 0 indicate that the pixel, patch, or image is nearer to its respective target space. improvement The improvement matrix quantifies the extent of improvement space. It is a matrix of dimensions HxW. The im_score parameter, defined in the compute method, acts as a modulator, allowing for fine-tuning of the space. omission The omission matrix, on the other hand, evaluates the extent of the omission space. This matrix provides insights into areas where the SR process might have failed to replicate crucial details from the HR image. hallucination The hallucination matrix measures the extent of artificial details or 'hallucinations' introduced in the SR image. The definition of the improvement and omission spaces conditions the determination of the hallucination space. classification The classification matrix is computed by applying a np.argmin function across the aforementioned correctness matrices. This matrix forms the basis for categorizing each pixel into one of the three classes: improvement, omission, and hallucination. auxiliary The auxiliary results in opensr-test comprise a set of intermediate outputs generated during the execution of the compute method. These results are instrumental in understanding the internal workings and transformations applied during the computation process. There are four key auxiliary results: sr_harm , lr_to_hr , matching_points_lr , and matching_points_hr . sr_harm This is the SR product post-harmonization. The harmonization pipeline is influenced by the harm_apply_spectral and harm_apply_spatial parameters. When both parameters are enabled (set to True), the SR image undergoes a two-step enhancement: first, the reflectance values are corrected via histogram matching with the HR image; subsequently, spatial alignment is performed, aligning the SR image with the HR image based on the settings defined in the spatial_features and spatial_matcher parameters. lr_to_hr This represents the LR image resampled to match the dimensions of the HR image. In the absence of a specific upsample_method set during the setup phase, the lr_to_hr result is achieved using a classic method - bilinear interpolation complemented by an anti-aliasing kernel filter. matching_points_lr These are the points of correspondence identified between the LR and HR images. This points help in understanding the spatial relationship and alignment between these two different tensor resolutions. matching_points_hr These are the points of correspondence identified between the SRharm and HR images. It provides insights into how well the SR image aligns with the HR image after the harmonization process.","title":"Results attributes"},{"location":"docs/API/results_attributes.html#_1","text":"","title":""},{"location":"docs/API/results_attributes.html#the-results-attribute","text":"The results attribute encapsulates all the metrics calculated at the agg_method level and the intermediate results produced by the opensr-test test. It organizes the output into four principal categories: consistency , distance , correctness , and auxiliary . The following example illustrates how to retrieve the outputs from the results attribute: # Import necessary libraries import torch import opensr_test # Generate sample LR, HR, and SR images lr = torch . rand ( 4 , 64 , 64 ) # Low Resolution image hr = torch . rand ( 4 , 256 , 256 ) # High Resolution image sr = torch . rand ( 4 , 256 , 256 ) # Super Resolution image # Initialize the Metrics object metrics = opensr_test . Metrics () # Compute the metrics metrics . compute ( lr = lr , sr = sr , hr = hr ) # Accessing Consistency Metrics metrics . results . consistency # Accessing Distance Metrics post SR Harmonization metrics . results . distance ## Accessing Correctness Metrics metrics . results . correctness # Accessing Intermediate Results metrics . results . auxiliary Except the auxiliary field, the spatial resolution of all the results is determined by the agg_method parameter. For instance, if agg_method is set to \"patch\" with a patch_size of 32, then each metric in the results attribute will contain values aggregated over patches of size H/32 x W/32.","title":"The results Attribute"},{"location":"docs/API/results_attributes.html#consistency","text":"The consistency metrics within opensr-test play a crucial role in evaluating the harmony between LR and SR images prior to SR harmonization (SRharm). These metrics are calculated after resampling the SR images to match the dimensions of the LR (SRdown). There are three key metrics in this category: reflectance , spectral , and spatial .","title":"consistency"},{"location":"docs/API/results_attributes.html#reflectance","text":"This metric evaluates how well the SR image reflects the norm values of the LR image. The calculation of reflectance consistency utilizes the method defined by the reflectance_method parameter.","title":"reflectance"},{"location":"docs/API/results_attributes.html#spectral","text":"This metric assesses the similarity in spectral characteristics between the LR and SR images. The computation of spectral consistency leverages the angle distance specified in the spectral_method parameter. This allows for a detailed comparison of the spectral profiles of the images, ensuring that the SR image preserves the original spectral properties of the LR image.","title":"spectral"},{"location":"docs/API/results_attributes.html#spatial","text":"The spatial consistency metric is computed by calculating the difference between the matching points identified in the LR and HR images. This process evaluates the spatial alignment and structural integrity of the SR image compared to the LR image. If the agg_method parameter is set to pixel or patch, the grid is calculated applying a simple kernel interpolation method.","title":"spatial"},{"location":"docs/API/results_attributes.html#distance","text":"distance metrics in opensr-test permit measure how far the SR image is from the LR and HR images. These metrics are computed post-harmonization (SRharm) to reduce the potential spatial and spectral bias introduced by the SR model. There are three distance metrics: sr_lr , sr_hr , and hr_lr .","title":"distance"},{"location":"docs/API/results_attributes.html#sr_to_lr-srharm-lr-distance","text":"This metric quantifies the distance between SRharm and the LR image. It serves as an indicator of how close the SR image is to the LR image. The LR is upsampled (LRup) to the dimensions of the HR image using the method defined by the upsample_method parameter. We strongly advise opting for a method that does not require parameter tuning, such as bilinear interpolation or other similar techniques. This is crucial to prevent the introduction of hallucination artifacts, which can significantly bias the experimental results.","title":"sr_to_lr (SRharm - LR Distance)"},{"location":"docs/API/results_attributes.html#sr_to_hr-srharm-hr-distance","text":"This metric measures the distance between the harmonized SR image (SRharm) and the HR image. It is essential for measure if the high-frequency details introduced by the SR model are consistent with the HR image (improvement) or if they are artificial (hallucination).","title":"sr_to_hr (SRharm - HR Distance)"},{"location":"docs/API/results_attributes.html#lr_to_hr-hr-lr-distance","text":"This metric calculates the distance between the HR and LR images. Although it doesn't directly involve the SR image, it offers a baseline understanding of the initial discrepancies between the HR and LR images, which can be useful for context and comparison.","title":"lr_to_hr (HR - LR Distance)"},{"location":"docs/API/results_attributes.html#correctness","text":"The correctness metrics in opensr-test are crucial assessments conducted after the SR harmonization process and the evaluation of the triple distance. These metrics encompass four categories: improvement , omission , hallucination , and classification . All the correctness metrics are designed such that values closer to 0 indicate that the pixel, patch, or image is nearer to its respective target space.","title":"correctness"},{"location":"docs/API/results_attributes.html#improvement","text":"The improvement matrix quantifies the extent of improvement space. It is a matrix of dimensions HxW. The im_score parameter, defined in the compute method, acts as a modulator, allowing for fine-tuning of the space.","title":"improvement"},{"location":"docs/API/results_attributes.html#omission","text":"The omission matrix, on the other hand, evaluates the extent of the omission space. This matrix provides insights into areas where the SR process might have failed to replicate crucial details from the HR image.","title":"omission"},{"location":"docs/API/results_attributes.html#hallucination","text":"The hallucination matrix measures the extent of artificial details or 'hallucinations' introduced in the SR image. The definition of the improvement and omission spaces conditions the determination of the hallucination space.","title":"hallucination"},{"location":"docs/API/results_attributes.html#classification","text":"The classification matrix is computed by applying a np.argmin function across the aforementioned correctness matrices. This matrix forms the basis for categorizing each pixel into one of the three classes: improvement, omission, and hallucination.","title":"classification"},{"location":"docs/API/results_attributes.html#auxiliary","text":"The auxiliary results in opensr-test comprise a set of intermediate outputs generated during the execution of the compute method. These results are instrumental in understanding the internal workings and transformations applied during the computation process. There are four key auxiliary results: sr_harm , lr_to_hr , matching_points_lr , and matching_points_hr .","title":"auxiliary"},{"location":"docs/API/results_attributes.html#sr_harm","text":"This is the SR product post-harmonization. The harmonization pipeline is influenced by the harm_apply_spectral and harm_apply_spatial parameters. When both parameters are enabled (set to True), the SR image undergoes a two-step enhancement: first, the reflectance values are corrected via histogram matching with the HR image; subsequently, spatial alignment is performed, aligning the SR image with the HR image based on the settings defined in the spatial_features and spatial_matcher parameters.","title":"sr_harm"},{"location":"docs/API/results_attributes.html#lr_to_hr","text":"This represents the LR image resampled to match the dimensions of the HR image. In the absence of a specific upsample_method set during the setup phase, the lr_to_hr result is achieved using a classic method - bilinear interpolation complemented by an anti-aliasing kernel filter.","title":"lr_to_hr"},{"location":"docs/API/results_attributes.html#matching_points_lr","text":"These are the points of correspondence identified between the LR and HR images. This points help in understanding the spatial relationship and alignment between these two different tensor resolutions.","title":"matching_points_lr"},{"location":"docs/API/results_attributes.html#matching_points_hr","text":"These are the points of correspondence identified between the SRharm and HR images. It provides insights into how well the SR image aligns with the HR image after the harmonization process.","title":"matching_points_hr"},{"location":"docs/Dataset/NAIP.html","text":"The NAIP product Identification Information Citation : Citation Information : Originator: USDA-FSA-APFO Aerial Photography Field Office Publication Date: 20111108 Title: NAIP Digital Ortho Photo Image Geospatial Data Presentation Form: remote-sensing image Publication Information : Publication Place: Salt Lake City, Utah Publisher: USDA-FSA-APFO Aerial Photography Field Office Description Abstract : This data set contains imagery from the National Agriculture Imagery Program (NAIP). NAIP acquires digital ortho imagery during the agricultural growing seasons in the continental U.S. A primary goal of the NAIP program is to make ortho imagery available within one year of acquisition. NAIP offers four main products: 1-meter GSD ortho imagery rectified to a horizontal accuracy of within +/- 5 meters of reference DOQQs from NDOP; 2-meter GSD ortho imagery rectified to within +/- 10 meters of reference DOQQs; 1-meter GSD ortho imagery rectified to within +/- 6 meters to true ground; and 2-meter GSD ortho imagery rectified to within +/- 10 meters to true ground. The tiling format of NAIP imagery is based on a 3.75' x 3.75' quarter quadrangle with a 300-meter buffer on all sides, formatted to the UTM coordinate system using NAD83. NAIP imagery may contain up to 10% cloud cover per tile. Purpose : NAIP imagery is intended to provide current agricultural conditions in support of USDA farm programs. The 1-meter GSD product is used for Common Land Unit boundaries and other data sets. The 1-meter NAIP imagery is usually acquired in full-state projects in cooperation with state government and other federal agencies for various purposes, including land use planning and natural resource assessment. NAIP is also used for disaster response, providing current pre-event imagery. The 2-meter GSD NAIP imagery, primarily for assessing crop condition and compliance with USDA farm program conditions, is typically acquired only for agricultural areas within state projects. Time Period of Content Time Period Information : Single Date/Time: Calendar Date: 20110720 Currentness Reference: Ground Condition Status Progress: Complete Maintenance and Update Frequency: Irregular Spatial Domain Bounding Coordinates : West Bounding Coordinate: -112.0000 East Bounding Coordinate: -111.9375 North Bounding Coordinate: 40.5625 South Bounding Coordinate: 40.5000 Keywords Theme : Theme Keyword Thesaurus: None Theme Keywords: farming, Digital Ortho rectified Image, Ortho Rectification, Quarter Quadrangle Centered, NAIP, Aerial Compliance, Compliance Place : Place Keyword Thesaurus: Geographic Names Information System Place Keywords: UT, Salt Lake, 49035, UT035, SALT LAKE CO UT FSA, 4011125, MIDVALE, SW, MIDVALE Access Constraints There are no limitations for access. Use Constraints Imagery may be replaced to address defects found in a small number of products through quality assurance processes. Imagery containing defects requiring new imagery acquisition, such as excessive cloud cover, specular reflectance, etc., will not be replaced within a NAIP project year. Point of Contact Contact Information : Contact Organization Primary : Contact Organization: Aerial Photography Field Office (APFO) Contact Address : Address Type: mailing and physical address Address: 2222 West 2300 South, Salt Lake City, Utah, 84119-2020, USA Contact Voice Telephone: 801-844-2922 Contact Facsimile Telephone: 801-956-3653 Contact Electronic Mail Address: apfo.sales@slc.usda.gov Browse Graphic Browse Graphic File Name: None Browse Graphic File Description: None Browse Graphic File Type: None Native Data Set Environment Unknown Data Quality Information Logical Consistency Report : NAIP 3.75 minute tile file names are based on the USGS quadrangle naming convention. Completeness Report: None Positional Accuracy : Horizontal Positional Accuracy : Horizontal Positional Accuracy Report: FSA Digital Orthophoto Specs. Lineage Source Information : Source Citation : Citation Information : Originator: USDA-FSA-APFO Aerial Photography Field Office Publication Date: 20111108 Title: MIDVALE, SW Geospatial Data Presentation Form: remote-sensing image Type of Source Media: Unknown Source Time Period of Content : Time Period Information : Single Date/Time: Calendar Date: 20110720 Source Currentness Reference: Aerial Photography Date for aerial photo source. Source Citation Abbreviation: Georectified Image Source Contribution: Digital Georectified Image. Process Step : Process Description: Digital imagery was collected at a nominal GSD of 1.0m using three Cessna 441 aircrafts flying at an average flight height of 9052m AGL. N14NW, N16NW flew with Leica Geosystem's ADS80/SH82 digital sensors sn30017 & sn30034 both with firmware 3.15. N911PJ flew with Leica Geosystem's ADS40/SH52 digital sensor sn30014 with firmware v2.14. Each sensor collected 11 image bands. PanF27A, PanF02A, and PanB14A panchromatic bands with a spectral range of 465-676nm. RedN00a and RedB16a with a spectral range of 604-664nm. GrnN00a and GrnB16a with a spectral range of 533-587nm. BluN00a and BluB16a with a spectral range of 420-492nm and Near-infrared bands NirN00a and NirB16a with a spectral range of 833-920nm. The CCD arrays have a pixel size of 6.5 microns in a 12000x1 format. Both the CCD's and the A/D convertors have a dynamic range of 12bits. The data is stored in 16bit format. The ADS is a push-broom sensor and the ground footprint of the imagery at NAIP scale is 12km wide by the length flightline. The maximum flightline length is limited to approximately 240km. The factory calibrations and IMU alignments for each sensor (sn30017 8/6/2009, sn30014 5/15/2008 and sn30034 12/7/2007) were tested and verified by in-situ test flights before the start of the project. The Leica ADS Flight Planning and Evaluation Software (FPES) is used to develop the flight acquisition plans. Flight acquisition sub blocks are designed first to define the GNSS base station logistics, and to break the project up into manageable acquisition units. The flight acquisition sub blocks are designed based on the specified acquisition season, native UTM zone of the DOQQs, flight line length limitations (to ensure sufficient performance of the IMU solution) as well as air traffic restrictions in the area. Once the sub blocks have been delineated they are brought into FPES for flight line design. The design parameters used in FPES will be 30% lateral overlap and 1.0m resolution. The flight lines have been designed with a north/south orientation. The design takes into account the latitude of the state, which affects line spacing due to convergence as well as the terrain. SRTM elevation data is used in the FPES design to ensure the 1m GSD is achieved over all types of terrain. The raw data was downloaded from the sensors after each flight using Leica XPro software. The imagery was then georeferenced using the 200Hz GPS/INS data creating an exterior orientation for each scan line (x/y/z/o/p/k). Technicians precisely measured tie points in 3 bands/looks (Back/Nadir/Forward) for each line using Leica Xpro software. The resulting point data and exterior orientation data were used to perform a full bundle adjustment with ORIMA software. Blunders were removed, and additional tie points measured in weak areas to ensure a robust solution. Once the point data was clean and point coverage was acceptable, photo-identifiable GPS-surveyed ground control points were introduced into the block adjustment. The bundle adjustment process produces revised exterior orientation data for the sensor with GPS/INS, datum, and sensor calibration errors modeled and removed. Using the revised exterior orientation from the bundle adjustment, orthorectified image strips were created with Xpro software and the May 2011 USGS 10m NED DEM. The Xpro orthorectification software applies an atmospheric-BRDF radiometric correction to the imagery. This correction compensates for atmospheric absorption, solar illumination angle and bi-directional reflectance. The orthorectified strips were then overlaid with each other and the ground control to check accuracy. Once the accuracy of the orthorectified image strips were validated the strips were then imported into Inpho's OrthoVista 4.5 package which was used for the final radiometric balance, mosaic, and DOQQ sheet creation. The final DOQQ sheets, with a 300m buffer and a ground pixel resolution of 1m were then combined and compressed to create the county-wide CCMs. Process Date: 20111108 Spatial Data Organization Information Indirect Spatial Reference: Salt Lake County, UT Direct Spatial Reference Method: Raster Raster Object Information : Raster Object Type: Pixel Row Count: 1 Column Count: 1 Spatial Reference Information Horizontal Coordinate System Definition : Planar : Grid Coordinate System : Grid Coordinate System Name: Universal Transverse Mercator Universal Transverse Mercator : UTM Zone Number: 12 Transverse Mercator : Scale Factor at Central Meridian: 0.9996 Longitude of Central Meridian: -111.0 Latitude of Projection Origin: 0.0 False Easting: 500000 False Northing: 0.0 Planar Coordinate Information : Planar Coordinate Encoding Method: row and column Coordinate Representation : Abscissa Resolution: 1 Ordinate Resolution: 1 Planar Distance Units: meters Geodetic Model : Horizontal Datum Name: North American Datum of 1983 Ellipsoid Name: Geodetic Reference System 80 (GRS 80) Semi-major Axis: 6378137 Denominator of Flattening Ratio: 298.257 Entity and Attribute Information Overview Description : Entity and Attribute Overview : 32-bit pixels, 4 band color (RGBIR) values 0 - 255 Entity and Attribute Detail Citation: None Distribution Information Distributor : Contact Information : Contact Person Primary : Contact Person: Supervisor Customer Services Section Contact Organization: USDA-FSA-APFO Aerial Photography Field Office Contact Address : Address Type: mailing and physical address Address: 2222 West 2300 South, Salt Lake City, Utah, 84119-2020, USA Contact Voice Telephone: 801-844-2922 Contact Facsimile Telephone: 801-956-3653 Contact Electronic Mail Address: apfo.sales@slc.usda.gov Distribution Liability : In no event shall the creators, custodians, or distributors of this information be liable for any damages arising out of its use (or the inability to use it). Standard Order Process : Digital Form : Digital Transfer Information : Format Name: GeoTIFF - Georeferenced Tagged Image File Format Format Information Content: Multispectral 4-band Digital Transfer Option : Offline Option : Offline Media: CD-ROM Recording Format: ISO 9660 Mode 1 Level 2 Extensions Fees: Contact the Aerial Photography Field Office for more information Resource Description: m_4011125_sw_12_1_20110720_20111011.tif Metadata Reference Information Metadata Date: 20111108 Metadata Contact : Contact Information : Contact Organization Primary : Contact Organization: USDA-FSA-APFO Aerial Photography Field Office Contact Address : Address Type: mailing and physical address Address: 2222 West 2300 South, Salt Lake City, Utah, 84119-2020, USA Contact Voice Telephone: 801-844-2922 Metadata Standard Name: Content Standard for Digital Geospatial Metadata Metadata Standard Version: FGDC-STD-001-1998 Metadata obtained from: https://naipeuwest.blob.core.windows.net/naip/v002/ut/2011/ut_fgdc_2011/40111/m_4011125_sw_12_1_20110720.txt","title":"NAIP"},{"location":"docs/Dataset/NAIP.html#_1","text":"","title":""},{"location":"docs/Dataset/NAIP.html#the-naip-product","text":"","title":"The NAIP product"},{"location":"docs/Dataset/NAIP.html#identification-information","text":"Citation : Citation Information : Originator: USDA-FSA-APFO Aerial Photography Field Office Publication Date: 20111108 Title: NAIP Digital Ortho Photo Image Geospatial Data Presentation Form: remote-sensing image Publication Information : Publication Place: Salt Lake City, Utah Publisher: USDA-FSA-APFO Aerial Photography Field Office","title":"Identification Information"},{"location":"docs/Dataset/NAIP.html#description","text":"Abstract : This data set contains imagery from the National Agriculture Imagery Program (NAIP). NAIP acquires digital ortho imagery during the agricultural growing seasons in the continental U.S. A primary goal of the NAIP program is to make ortho imagery available within one year of acquisition. NAIP offers four main products: 1-meter GSD ortho imagery rectified to a horizontal accuracy of within +/- 5 meters of reference DOQQs from NDOP; 2-meter GSD ortho imagery rectified to within +/- 10 meters of reference DOQQs; 1-meter GSD ortho imagery rectified to within +/- 6 meters to true ground; and 2-meter GSD ortho imagery rectified to within +/- 10 meters to true ground. The tiling format of NAIP imagery is based on a 3.75' x 3.75' quarter quadrangle with a 300-meter buffer on all sides, formatted to the UTM coordinate system using NAD83. NAIP imagery may contain up to 10% cloud cover per tile. Purpose : NAIP imagery is intended to provide current agricultural conditions in support of USDA farm programs. The 1-meter GSD product is used for Common Land Unit boundaries and other data sets. The 1-meter NAIP imagery is usually acquired in full-state projects in cooperation with state government and other federal agencies for various purposes, including land use planning and natural resource assessment. NAIP is also used for disaster response, providing current pre-event imagery. The 2-meter GSD NAIP imagery, primarily for assessing crop condition and compliance with USDA farm program conditions, is typically acquired only for agricultural areas within state projects.","title":"Description"},{"location":"docs/Dataset/NAIP.html#time-period-of-content","text":"Time Period Information : Single Date/Time: Calendar Date: 20110720 Currentness Reference: Ground Condition","title":"Time Period of Content"},{"location":"docs/Dataset/NAIP.html#status","text":"Progress: Complete Maintenance and Update Frequency: Irregular","title":"Status"},{"location":"docs/Dataset/NAIP.html#spatial-domain","text":"Bounding Coordinates : West Bounding Coordinate: -112.0000 East Bounding Coordinate: -111.9375 North Bounding Coordinate: 40.5625 South Bounding Coordinate: 40.5000","title":"Spatial Domain"},{"location":"docs/Dataset/NAIP.html#keywords","text":"Theme : Theme Keyword Thesaurus: None Theme Keywords: farming, Digital Ortho rectified Image, Ortho Rectification, Quarter Quadrangle Centered, NAIP, Aerial Compliance, Compliance Place : Place Keyword Thesaurus: Geographic Names Information System Place Keywords: UT, Salt Lake, 49035, UT035, SALT LAKE CO UT FSA, 4011125, MIDVALE, SW, MIDVALE","title":"Keywords"},{"location":"docs/Dataset/NAIP.html#access-constraints","text":"There are no limitations for access.","title":"Access Constraints"},{"location":"docs/Dataset/NAIP.html#use-constraints","text":"Imagery may be replaced to address defects found in a small number of products through quality assurance processes. Imagery containing defects requiring new imagery acquisition, such as excessive cloud cover, specular reflectance, etc., will not be replaced within a NAIP project year.","title":"Use Constraints"},{"location":"docs/Dataset/NAIP.html#point-of-contact","text":"Contact Information : Contact Organization Primary : Contact Organization: Aerial Photography Field Office (APFO) Contact Address : Address Type: mailing and physical address Address: 2222 West 2300 South, Salt Lake City, Utah, 84119-2020, USA Contact Voice Telephone: 801-844-2922 Contact Facsimile Telephone: 801-956-3653 Contact Electronic Mail Address: apfo.sales@slc.usda.gov","title":"Point of Contact"},{"location":"docs/Dataset/NAIP.html#browse-graphic","text":"Browse Graphic File Name: None Browse Graphic File Description: None Browse Graphic File Type: None","title":"Browse Graphic"},{"location":"docs/Dataset/NAIP.html#native-data-set-environment","text":"Unknown","title":"Native Data Set Environment"},{"location":"docs/Dataset/NAIP.html#data-quality-information","text":"Logical Consistency Report : NAIP 3.75 minute tile file names are based on the USGS quadrangle naming convention. Completeness Report: None Positional Accuracy : Horizontal Positional Accuracy : Horizontal Positional Accuracy Report: FSA Digital Orthophoto Specs.","title":"Data Quality Information"},{"location":"docs/Dataset/NAIP.html#lineage","text":"Source Information : Source Citation : Citation Information : Originator: USDA-FSA-APFO Aerial Photography Field Office Publication Date: 20111108 Title: MIDVALE, SW Geospatial Data Presentation Form: remote-sensing image Type of Source Media: Unknown Source Time Period of Content : Time Period Information : Single Date/Time: Calendar Date: 20110720 Source Currentness Reference: Aerial Photography Date for aerial photo source. Source Citation Abbreviation: Georectified Image Source Contribution: Digital Georectified Image. Process Step : Process Description: Digital imagery was collected at a nominal GSD of 1.0m using three Cessna 441 aircrafts flying at an average flight height of 9052m AGL. N14NW, N16NW flew with Leica Geosystem's ADS80/SH82 digital sensors sn30017 & sn30034 both with firmware 3.15. N911PJ flew with Leica Geosystem's ADS40/SH52 digital sensor sn30014 with firmware v2.14. Each sensor collected 11 image bands. PanF27A, PanF02A, and PanB14A panchromatic bands with a spectral range of 465-676nm. RedN00a and RedB16a with a spectral range of 604-664nm. GrnN00a and GrnB16a with a spectral range of 533-587nm. BluN00a and BluB16a with a spectral range of 420-492nm and Near-infrared bands NirN00a and NirB16a with a spectral range of 833-920nm. The CCD arrays have a pixel size of 6.5 microns in a 12000x1 format. Both the CCD's and the A/D convertors have a dynamic range of 12bits. The data is stored in 16bit format. The ADS is a push-broom sensor and the ground footprint of the imagery at NAIP scale is 12km wide by the length flightline. The maximum flightline length is limited to approximately 240km. The factory calibrations and IMU alignments for each sensor (sn30017 8/6/2009, sn30014 5/15/2008 and sn30034 12/7/2007) were tested and verified by in-situ test flights before the start of the project. The Leica ADS Flight Planning and Evaluation Software (FPES) is used to develop the flight acquisition plans. Flight acquisition sub blocks are designed first to define the GNSS base station logistics, and to break the project up into manageable acquisition units. The flight acquisition sub blocks are designed based on the specified acquisition season, native UTM zone of the DOQQs, flight line length limitations (to ensure sufficient performance of the IMU solution) as well as air traffic restrictions in the area. Once the sub blocks have been delineated they are brought into FPES for flight line design. The design parameters used in FPES will be 30% lateral overlap and 1.0m resolution. The flight lines have been designed with a north/south orientation. The design takes into account the latitude of the state, which affects line spacing due to convergence as well as the terrain. SRTM elevation data is used in the FPES design to ensure the 1m GSD is achieved over all types of terrain. The raw data was downloaded from the sensors after each flight using Leica XPro software. The imagery was then georeferenced using the 200Hz GPS/INS data creating an exterior orientation for each scan line (x/y/z/o/p/k). Technicians precisely measured tie points in 3 bands/looks (Back/Nadir/Forward) for each line using Leica Xpro software. The resulting point data and exterior orientation data were used to perform a full bundle adjustment with ORIMA software. Blunders were removed, and additional tie points measured in weak areas to ensure a robust solution. Once the point data was clean and point coverage was acceptable, photo-identifiable GPS-surveyed ground control points were introduced into the block adjustment. The bundle adjustment process produces revised exterior orientation data for the sensor with GPS/INS, datum, and sensor calibration errors modeled and removed. Using the revised exterior orientation from the bundle adjustment, orthorectified image strips were created with Xpro software and the May 2011 USGS 10m NED DEM. The Xpro orthorectification software applies an atmospheric-BRDF radiometric correction to the imagery. This correction compensates for atmospheric absorption, solar illumination angle and bi-directional reflectance. The orthorectified strips were then overlaid with each other and the ground control to check accuracy. Once the accuracy of the orthorectified image strips were validated the strips were then imported into Inpho's OrthoVista 4.5 package which was used for the final radiometric balance, mosaic, and DOQQ sheet creation. The final DOQQ sheets, with a 300m buffer and a ground pixel resolution of 1m were then combined and compressed to create the county-wide CCMs. Process Date: 20111108","title":"Lineage"},{"location":"docs/Dataset/NAIP.html#spatial-data-organization-information","text":"Indirect Spatial Reference: Salt Lake County, UT Direct Spatial Reference Method: Raster Raster Object Information : Raster Object Type: Pixel Row Count: 1 Column Count: 1","title":"Spatial Data Organization Information"},{"location":"docs/Dataset/NAIP.html#spatial-reference-information","text":"Horizontal Coordinate System Definition : Planar : Grid Coordinate System : Grid Coordinate System Name: Universal Transverse Mercator Universal Transverse Mercator : UTM Zone Number: 12 Transverse Mercator : Scale Factor at Central Meridian: 0.9996 Longitude of Central Meridian: -111.0 Latitude of Projection Origin: 0.0 False Easting: 500000 False Northing: 0.0 Planar Coordinate Information : Planar Coordinate Encoding Method: row and column Coordinate Representation : Abscissa Resolution: 1 Ordinate Resolution: 1 Planar Distance Units: meters Geodetic Model : Horizontal Datum Name: North American Datum of 1983 Ellipsoid Name: Geodetic Reference System 80 (GRS 80) Semi-major Axis: 6378137 Denominator of Flattening Ratio: 298.257","title":"Spatial Reference Information"},{"location":"docs/Dataset/NAIP.html#entity-and-attribute-information","text":"Overview Description : Entity and Attribute Overview : 32-bit pixels, 4 band color (RGBIR) values 0 - 255 Entity and Attribute Detail Citation: None","title":"Entity and Attribute Information"},{"location":"docs/Dataset/NAIP.html#distribution-information","text":"Distributor : Contact Information : Contact Person Primary : Contact Person: Supervisor Customer Services Section Contact Organization: USDA-FSA-APFO Aerial Photography Field Office Contact Address : Address Type: mailing and physical address Address: 2222 West 2300 South, Salt Lake City, Utah, 84119-2020, USA Contact Voice Telephone: 801-844-2922 Contact Facsimile Telephone: 801-956-3653 Contact Electronic Mail Address: apfo.sales@slc.usda.gov Distribution Liability : In no event shall the creators, custodians, or distributors of this information be liable for any damages arising out of its use (or the inability to use it). Standard Order Process : Digital Form : Digital Transfer Information : Format Name: GeoTIFF - Georeferenced Tagged Image File Format Format Information Content: Multispectral 4-band Digital Transfer Option : Offline Option : Offline Media: CD-ROM Recording Format: ISO 9660 Mode 1 Level 2 Extensions Fees: Contact the Aerial Photography Field Office for more information Resource Description: m_4011125_sw_12_1_20110720_20111011.tif","title":"Distribution Information"},{"location":"docs/Dataset/NAIP.html#metadata-reference-information","text":"Metadata Date: 20111108 Metadata Contact : Contact Information : Contact Organization Primary : Contact Organization: USDA-FSA-APFO Aerial Photography Field Office Contact Address : Address Type: mailing and physical address Address: 2222 West 2300 South, Salt Lake City, Utah, 84119-2020, USA Contact Voice Telephone: 801-844-2922 Metadata Standard Name: Content Standard for Digital Geospatial Metadata Metadata Standard Version: FGDC-STD-001-1998 Metadata obtained from: https://naipeuwest.blob.core.windows.net/naip/v002/ut/2011/ut_fgdc_2011/40111/m_4011125_sw_12_1_20110720.txt","title":"Metadata Reference Information"},{"location":"docs/Dataset/SPOT.html","text":"The SPOT product The SPOT product were obtained from the Worldstrat dataset. The RGBN bands were downsampled to 1.5 m/pixel using the band pan-sharpening method defined in the worldstrat repository, and then to 2.5 m/pixel using bilinear interpolation with the intention to provide a 4x super-resolution factor. Due to the irreversible nature of the pansharpening method, which is theoretically applicable primarily to RGB bands, a significant degradation of spectral information occurs in most images within the worldstrat collection. As a result of this degradation, only 12 Regions of Interest (ROIs) have retained sufficient quality.","title":"SPOT"},{"location":"docs/Dataset/SPOT.html#_1","text":"","title":""},{"location":"docs/Dataset/SPOT.html#the-spot-product","text":"The SPOT product were obtained from the Worldstrat dataset. The RGBN bands were downsampled to 1.5 m/pixel using the band pan-sharpening method defined in the worldstrat repository, and then to 2.5 m/pixel using bilinear interpolation with the intention to provide a 4x super-resolution factor. Due to the irreversible nature of the pansharpening method, which is theoretically applicable primarily to RGB bands, a significant degradation of spectral information occurs in most images within the worldstrat collection. As a result of this degradation, only 12 Regions of Interest (ROIs) have retained sufficient quality.","title":"The SPOT product"},{"location":"docs/Dataset/Ven%C2%B5s.html","text":"The Ven\u00b5s product The Ven\u00b5s images featured in the study were sourced from the Sen2Ven\u00b5s dataset. Notably, as of the publication of this article, it is the only dataset that offers a genuine harmonization pipeline. More details about the harmonization process can be found in the Sen2Ven\u00b5s paper .","title":"Ven\u00b5s"},{"location":"docs/Dataset/Ven%C2%B5s.html#_1","text":"","title":""},{"location":"docs/Dataset/Ven%C2%B5s.html#the-vens-product","text":"The Ven\u00b5s images featured in the study were sourced from the Sen2Ven\u00b5s dataset. Notably, as of the publication of this article, it is the only dataset that offers a genuine harmonization pipeline. More details about the harmonization process can be found in the Sen2Ven\u00b5s paper .","title":"The Ven\u00b5s product"},{"location":"docs/Dataset/main.html","text":"The opensr-test Dataset The opensr-test dataset was meticulously prepared with the primary aim of maintaining the maximum possible consistency between the LR and HR pair images. The entire process can be divided into three steps: selection of potential LR-HR pairs, harmonization, and visual inspection, as depicted in Figure above. For the LR-HR pair selection, we utilized Sentinel-2 L2A as the LR image and three different pre-processed HR sources: NAIP, SPOT and Venus. In order to ensure similar atmospheric conditions, we started to discard all the LR-HR pairs that did not count on the same day. Additionally, LR-HR pairs with over 0 % cloud cover in the LR image were automatically discarded using a cloud detection algorithm trained on CloudSEN12. The final dataset characteristics are summarized in the table below. Dataset Scale # Scenes HRsize HR Reference NAIP x4 30 512 USDA Farm Production and Conservation - Business Center, Geospatial Enterprise Operations. SPOT x4 12 512 European Space Agency, 2017, SPOT 1-5 ESA Mini-SEN2VEN\u00b5S x2 59 512 Vegetation and Environment monitoring on a New Micro-Satellite (SEN2VEN\u03bcS). The opensr-test dataset API provides a simple interface to download the dataset. All the dataset are stored in the HuggingFace Datasets Repository https://huggingface.co/csaybar/opensr-test. The following code snippet shows how to download the dataset and load it into memory. import opensr_test # Load the spot dataset lr , hr , landuse , parameters = opensr_test . load ( \"spot\" ) . values () # Load the venus dataset lr , hr , landuse , parameters = opensr_test . load ( \"venus\" ) . values () # Load the naip dataset lr , hr , landuse , parameters = opensr_test . load ( \"naip\" ) . values () opensr_test.load Function The opensr_test.load function is a key utility in the opensr-test package, offering a streamlined solution for downloading and loading datasets. It returns a dictionary containing the following keys: lr : LR images sourced from Sentinel-2 L2A, representing the base resolution for analysis. hr : HR images that are harmonized in reference to the LR images. These HR images can br from sources like NAIP, SPOT, or VENUS according to the dataset parameter. landuse : Land use classification images, derived from the ESA Land Cover map, providing contextual information about the geographical area covered by the LR and HR images. parameters : A datamodel encompassing various fields that guide image processing and analysis. These include: - blur_gaussian_sigma : Specifies the standard deviation for a Gaussian 2D filter, which can be used to blur the hr images. This parameter is fine-tuned comparing the LR and HR images, optimized against the L1 loss metric. - stability_threshold : A threshold value used to differentiate between stable and unstable pixels. Stable pixels are those that exhibit minimal or no change between the lr and hr images. This threshold is established through visual inspections performed by three remote sensing experts on each image. - correctness_params : Global parameters delineating the spaces for omission, improvement and hallucinations. These are determined through the expert analysis of three different specialists, ensuring a rigorous and comprehensive evaluation. - downsample_method : The recommended approach for downsampling the hr images to a lower resolution. - upsample_method : The suggested technique for upsampling the lr images to a higher resolution.","title":"Home"},{"location":"docs/Dataset/main.html#_1","text":"","title":""},{"location":"docs/Dataset/main.html#the-opensr-test-dataset","text":"The opensr-test dataset was meticulously prepared with the primary aim of maintaining the maximum possible consistency between the LR and HR pair images. The entire process can be divided into three steps: selection of potential LR-HR pairs, harmonization, and visual inspection, as depicted in Figure above. For the LR-HR pair selection, we utilized Sentinel-2 L2A as the LR image and three different pre-processed HR sources: NAIP, SPOT and Venus. In order to ensure similar atmospheric conditions, we started to discard all the LR-HR pairs that did not count on the same day. Additionally, LR-HR pairs with over 0 % cloud cover in the LR image were automatically discarded using a cloud detection algorithm trained on CloudSEN12. The final dataset characteristics are summarized in the table below. Dataset Scale # Scenes HRsize HR Reference NAIP x4 30 512 USDA Farm Production and Conservation - Business Center, Geospatial Enterprise Operations. SPOT x4 12 512 European Space Agency, 2017, SPOT 1-5 ESA Mini-SEN2VEN\u00b5S x2 59 512 Vegetation and Environment monitoring on a New Micro-Satellite (SEN2VEN\u03bcS). The opensr-test dataset API provides a simple interface to download the dataset. All the dataset are stored in the HuggingFace Datasets Repository https://huggingface.co/csaybar/opensr-test. The following code snippet shows how to download the dataset and load it into memory. import opensr_test # Load the spot dataset lr , hr , landuse , parameters = opensr_test . load ( \"spot\" ) . values () # Load the venus dataset lr , hr , landuse , parameters = opensr_test . load ( \"venus\" ) . values () # Load the naip dataset lr , hr , landuse , parameters = opensr_test . load ( \"naip\" ) . values ()","title":"The opensr-test Dataset"},{"location":"docs/Dataset/main.html#opensr_testload-function","text":"The opensr_test.load function is a key utility in the opensr-test package, offering a streamlined solution for downloading and loading datasets. It returns a dictionary containing the following keys: lr : LR images sourced from Sentinel-2 L2A, representing the base resolution for analysis. hr : HR images that are harmonized in reference to the LR images. These HR images can br from sources like NAIP, SPOT, or VENUS according to the dataset parameter. landuse : Land use classification images, derived from the ESA Land Cover map, providing contextual information about the geographical area covered by the LR and HR images. parameters : A datamodel encompassing various fields that guide image processing and analysis. These include: - blur_gaussian_sigma : Specifies the standard deviation for a Gaussian 2D filter, which can be used to blur the hr images. This parameter is fine-tuned comparing the LR and HR images, optimized against the L1 loss metric. - stability_threshold : A threshold value used to differentiate between stable and unstable pixels. Stable pixels are those that exhibit minimal or no change between the lr and hr images. This threshold is established through visual inspections performed by three remote sensing experts on each image. - correctness_params : Global parameters delineating the spaces for omission, improvement and hallucinations. These are determined through the expert analysis of three different specialists, ensuring a rigorous and comprehensive evaluation. - downsample_method : The recommended approach for downsampling the hr images to a lower resolution. - upsample_method : The suggested technique for upsampling the lr images to a higher resolution.","title":"opensr_test.load Function"},{"location":"docs/Metrics/consistency.html","text":"Consistency metrics The consistency metrics are used to evaluate the ability of the super-resolution model to preserve the spectral and spatial information of the LR image. The opensr-test package provides three different metrics to evaluate the consistency of the super-resolution model: reflectance, spectral, and spatial. See the result attributes for more information about how to retrieve the outputs. The reflectance metrics are employed to assess the impact of the SR model on the norm of reflectance values. The distance metric supported are: l1, l2, pbias, and kl. The spectral metrics are used to evaluate if the SR model is preserving the spectral profiles of the images. The only distance metric supported are: sad. The spatial metrics are used to evaluate the spatial alignment and structural integrity of the SR image compared to the LR image. The only distance metric supported are: ligthglue+superpoint and lightglue+disk. L1 distance : The L1 distance is the sum of the absolute differences between the two vectors. It is also known as the Manhattan distance. \\(L1(y, \u0177) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \u0177_i|\\) L2 distance : The L2 distance is the square root of the sum of the squared differences between the two vectors. It is also known as the Euclidean distance. \\(L2(y, \u0177) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \u0177_i)^2\\) Spectral angle distance : The spectral angle distance is the angle between two vectors. This metric is used to measure the spectral consistency. \\(SAD(\\vec{y}, \\vec{\\hat{y}}) = \\arccos\\left(\\frac{\\vec{y} \\cdot \\vec{\\hat{y}}}{\\|\\vec{y}\\| \\|\\vec{\\hat{y}}\\|}\\right)\\) Percentage Bias : The Percentage Bias (PBIAS) measures the average tendency of the super-resolved values to be larger or smaller than their observed counterparts. This metric help us to understand whether a model is changing the norm of the original reflectance values. The equation for calculating Percentage Bias is: \\(PBIAS = \\left( \\frac{\\sum_{i=1}^{n} (O_i - S_i)}{\\sum_{i=1}^{n} O_i} \\right)\\) Inverted Peak Signal-to-Noise Ratio : The Inverted Peak Signal-to-Noise Ratio (IPSNR) is the inverse of the Peak Signal-to-Noise Ratio (PSNR). This metric is used to measure the high-frequency information. The equation for calculating IPSNR is: \\(IPSNR = \\frac{1}{PSNR}\\) Kullback-Leibler divergence : The Kullback-Leibler divergence (KLD) is a measure of how one probability distribution is different from a second, reference probability distribution. This metric is used to measure the high-frequency information. The equation for calculating KLD is: \\(KL(P || Q) = \\sum_{x} P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right)\\)","title":"Consistency metrics"},{"location":"docs/Metrics/consistency.html#consistency-metrics","text":"The consistency metrics are used to evaluate the ability of the super-resolution model to preserve the spectral and spatial information of the LR image. The opensr-test package provides three different metrics to evaluate the consistency of the super-resolution model: reflectance, spectral, and spatial. See the result attributes for more information about how to retrieve the outputs. The reflectance metrics are employed to assess the impact of the SR model on the norm of reflectance values. The distance metric supported are: l1, l2, pbias, and kl. The spectral metrics are used to evaluate if the SR model is preserving the spectral profiles of the images. The only distance metric supported are: sad. The spatial metrics are used to evaluate the spatial alignment and structural integrity of the SR image compared to the LR image. The only distance metric supported are: ligthglue+superpoint and lightglue+disk. L1 distance : The L1 distance is the sum of the absolute differences between the two vectors. It is also known as the Manhattan distance. \\(L1(y, \u0177) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \u0177_i|\\) L2 distance : The L2 distance is the square root of the sum of the squared differences between the two vectors. It is also known as the Euclidean distance. \\(L2(y, \u0177) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \u0177_i)^2\\) Spectral angle distance : The spectral angle distance is the angle between two vectors. This metric is used to measure the spectral consistency. \\(SAD(\\vec{y}, \\vec{\\hat{y}}) = \\arccos\\left(\\frac{\\vec{y} \\cdot \\vec{\\hat{y}}}{\\|\\vec{y}\\| \\|\\vec{\\hat{y}}\\|}\\right)\\) Percentage Bias : The Percentage Bias (PBIAS) measures the average tendency of the super-resolved values to be larger or smaller than their observed counterparts. This metric help us to understand whether a model is changing the norm of the original reflectance values. The equation for calculating Percentage Bias is: \\(PBIAS = \\left( \\frac{\\sum_{i=1}^{n} (O_i - S_i)}{\\sum_{i=1}^{n} O_i} \\right)\\) Inverted Peak Signal-to-Noise Ratio : The Inverted Peak Signal-to-Noise Ratio (IPSNR) is the inverse of the Peak Signal-to-Noise Ratio (PSNR). This metric is used to measure the high-frequency information. The equation for calculating IPSNR is: \\(IPSNR = \\frac{1}{PSNR}\\) Kullback-Leibler divergence : The Kullback-Leibler divergence (KLD) is a measure of how one probability distribution is different from a second, reference probability distribution. This metric is used to measure the high-frequency information. The equation for calculating KLD is: \\(KL(P || Q) = \\sum_{x} P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right)\\)","title":"Consistency metrics"},{"location":"docs/Metrics/correctness.html","text":"The correctness metrics The correctness metrics evaluate the quality of high-frequency information introduced by the SR model. Three correctness metrics are implemented in opensr-test : improvement, omission, and hallucination. Each metric serves a specific purpose: Improvement : Low values represent a good match between the SR and HR images. The equation for calculating improvement is: \\(H = d_{im} + d_{om} - 1\\) \\(Improvement = d_{im} + d_{om}*(1 - e^{-\\gamma H})\\) Where: \\(d_{im}\\) is the distance between the SR and HR images. \\(d_{om}\\) is the distance between the SR and LR images. Omission : Low values are related to the inability to represent the actual high-frequency information from the landscape and keep similar to the LR image. The equation for calculating improvement is: \\(H = d_{im} + d_{om} - 1\\) \\(Omission = d_{om} + d_{im}*(1 - e^{-\\gamma H})\\) Hallucination : Low values shows the areas where the SR model has introduced high-frequency information that is not present in the HR image. The equation for calculating hallucination is: \\(Hallucination = e^{-\\gamma * d_{om} * d_{im}}\\)","title":"Correctness metrics"},{"location":"docs/Metrics/correctness.html#_1","text":"","title":""},{"location":"docs/Metrics/correctness.html#the-correctness-metrics","text":"The correctness metrics evaluate the quality of high-frequency information introduced by the SR model. Three correctness metrics are implemented in opensr-test : improvement, omission, and hallucination. Each metric serves a specific purpose: Improvement : Low values represent a good match between the SR and HR images. The equation for calculating improvement is: \\(H = d_{im} + d_{om} - 1\\) \\(Improvement = d_{im} + d_{om}*(1 - e^{-\\gamma H})\\) Where: \\(d_{im}\\) is the distance between the SR and HR images. \\(d_{om}\\) is the distance between the SR and LR images. Omission : Low values are related to the inability to represent the actual high-frequency information from the landscape and keep similar to the LR image. The equation for calculating improvement is: \\(H = d_{im} + d_{om} - 1\\) \\(Omission = d_{om} + d_{im}*(1 - e^{-\\gamma H})\\) Hallucination : Low values shows the areas where the SR model has introduced high-frequency information that is not present in the HR image. The equation for calculating hallucination is: \\(Hallucination = e^{-\\gamma * d_{om} * d_{im}}\\)","title":"The correctness metrics"},{"location":"docs/Metrics/distance.html","text":"Triple distance metrics The triple distance metrics are used to evaluate if the SR product is closer to the HR product or to the LR product. The distance metrics are calculated two times: once between the SR and HR images, and once between the SR and LR images. Apart from the metrics described in the consistency metrics, we also introduce the LPIPS and CLIP metrics. LPIPS : The Learned Perceptual Image Patch Similarity (LPIPS) metric is a perceptual metric that aims to quantify the perceptual similarity between two images. The LPIPS metric is based on a deep neural network that was trained to predict perceptual similarity scores. The reported metric is the average LPIPS score between the LR and SR images. \\(\\text{LPIPS} = \\sum_{l=1}^{L} w_l \\cdot \\frac{1}{H_lW_l} \\sum_{h=1}^{H_l} \\sum_{w=1}^{W_l} \\| \\phi_l(I_1)_{h,w} - \\phi_l(I_2)_{h,w} \\|_2^2\\) In this equation: \\(LPIPS\\) is the Learned Perceptual Image Patch Similarity score. \\(L\\) denotes the number of layers in a deep neural network used for comparison. \\(w_l\\) represents the weight of the \\(l-th\\) layer in the network. \\(\\phi_l(l)_{I}\\) is the feature map of image \\(I\\) at layer \\(l\\) . \\(H_l\\) and \\(W_l\\) are the height and width of the feature map at layer \\(l\\) , respectively. \\(I_1\\) and \\(I_2\\) are the two images being compared. The summations across \\(h\\) and \\(w\\) are over the spatial dimensions of the feature maps. CLIPscore Proposed by Wolters et. al 2023 in Zooming Out on Zooming In: Advancing Super-Resolution for Remote Sensing . CLIPscore measures the distance in image embedding space between the HR|LR and the SR image using the CLIP model. Unlike LPIPS, with CLIPscore we can focus mainly in the contextual and semantic integrity of the super-resolved images. The reported metric is the cosine similarity between the image embeddings of the HR|LR and SR images.","title":"Distance metrics"},{"location":"docs/Metrics/distance.html#_1","text":"","title":""},{"location":"docs/Metrics/distance.html#triple-distance-metrics","text":"The triple distance metrics are used to evaluate if the SR product is closer to the HR product or to the LR product. The distance metrics are calculated two times: once between the SR and HR images, and once between the SR and LR images. Apart from the metrics described in the consistency metrics, we also introduce the LPIPS and CLIP metrics. LPIPS : The Learned Perceptual Image Patch Similarity (LPIPS) metric is a perceptual metric that aims to quantify the perceptual similarity between two images. The LPIPS metric is based on a deep neural network that was trained to predict perceptual similarity scores. The reported metric is the average LPIPS score between the LR and SR images. \\(\\text{LPIPS} = \\sum_{l=1}^{L} w_l \\cdot \\frac{1}{H_lW_l} \\sum_{h=1}^{H_l} \\sum_{w=1}^{W_l} \\| \\phi_l(I_1)_{h,w} - \\phi_l(I_2)_{h,w} \\|_2^2\\) In this equation: \\(LPIPS\\) is the Learned Perceptual Image Patch Similarity score. \\(L\\) denotes the number of layers in a deep neural network used for comparison. \\(w_l\\) represents the weight of the \\(l-th\\) layer in the network. \\(\\phi_l(l)_{I}\\) is the feature map of image \\(I\\) at layer \\(l\\) . \\(H_l\\) and \\(W_l\\) are the height and width of the feature map at layer \\(l\\) , respectively. \\(I_1\\) and \\(I_2\\) are the two images being compared. The summations across \\(h\\) and \\(w\\) are over the spatial dimensions of the feature maps. CLIPscore Proposed by Wolters et. al 2023 in Zooming Out on Zooming In: Advancing Super-Resolution for Remote Sensing . CLIPscore measures the distance in image embedding space between the HR|LR and the SR image using the CLIP model. Unlike LPIPS, with CLIPscore we can focus mainly in the contextual and semantic integrity of the super-resolved images. The reported metric is the cosine similarity between the image embeddings of the HR|LR and SR images.","title":"Triple distance metrics"}]}