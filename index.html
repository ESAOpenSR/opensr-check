
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A comprehensive benchmark for real-world Sentinel-2 imagery super-resolution">
      
      
        <meta name="author" content="Open SR test Contributors">
      
      
        <link rel="canonical" href="https://github.com/ESAOpenSR/opensr-test/index.html">
      
      <link rel="icon" href="resources/ms_icon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.2.16">
    
    
      
        <title>Index - OpenSR test</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.1c3799f8.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.cc9b2e1e.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL(".",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="black">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="OpenSR test" class="md-header__button md-logo" aria-label="OpenSR test" data-md-component="logo">
      
  <img src="docs/images/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            OpenSR test
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Index
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="#201357" data-md-color-accent="white"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ESAOpenSR/opensr-test" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    OpenSR test
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="index.html" class="md-tabs__link md-tabs__link--active">
        Home
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="docs/API/config_pydantic.html" class="md-tabs__link">
        API
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="docs/Metrics/distance.html" class="md-tabs__link">
        Metrics
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="OpenSR test" class="md-nav__button md-logo" aria-label="OpenSR test" data-md-component="logo">
      
  <img src="docs/images/logo.png" alt="logo">

    </a>
    OpenSR test
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ESAOpenSR/opensr-test" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    OpenSR test
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" checked>
      
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index md-nav__link--active">
          <a href="index.html">Home</a>
          
            <label for="__nav_1">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Home" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Home
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="docs/CONTRIBUTING.html" class="md-nav__link">
        Contributing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="docs/Code_of_Conduct.html" class="md-nav__link">
        Code of Conduct
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="docs/CHANGELOG.html" class="md-nav__link">
        Changelog
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="docs/LICENSE.html" class="md-nav__link">
        License
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" data-md-state="indeterminate" type="checkbox" id="__nav_2" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2">
          API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="API" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="docs/API/config_pydantic.html" class="md-nav__link">
        Config
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="docs/API/compute_method.html" class="md-nav__link">
        Compute method
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="docs/API/results_attributes.html" class="md-nav__link">
        Results attributes
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" data-md-state="indeterminate" type="checkbox" id="__nav_3" checked>
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3">
          Metrics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Metrics" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Metrics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="docs/Metrics/distance.html" class="md-nav__link">
        Distance metrics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="docs/Metrics/correctness.html" class="md-nav__link">
        Correctness scores
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-use" class="md-nav__link">
    How to use
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#benchmark" class="md-nav__link">
    Benchmark
  </a>
  
    <nav class="md-nav" aria-label="Benchmark">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clip" class="md-nav__link">
    CLIP
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lpips" class="md-nav__link">
    LPIPS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#normalized-difference-nd" class="md-nav__link">
    Normalized Difference (ND)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    Installation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#datasets" class="md-nav__link">
    Datasets
  </a>
  
    <nav class="md-nav" aria-label="Datasets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#naip-x4-scale-factor" class="md-nav__link">
    NAIP (X4 scale factor)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spot-x4-scale-factor" class="md-nav__link">
    SPOT (X4 scale factor)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vens-x2-scale-factor" class="md-nav__link">
    Venµs (X2 scale factor)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spain-crops-x4-scale-factor" class="md-nav__link">
    SPAIN CROPS (x4 scale factor)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spain-urban-x4-scale-factor" class="md-nav__link">
    SPAIN URBAN (x4 scale factor)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#examples" class="md-nav__link">
    Examples
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#visualizations" class="md-nav__link">
    Visualizations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deeper-understanding" class="md-nav__link">
    Deeper understanding
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#citation" class="md-nav__link">
    Citation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#acknowledgements" class="md-nav__link">
    Acknowledgements
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
  <a href="https://github.com/ESAOpenSR/opensr-test/edit/master/docs/README.md" title="Edit this page" class="md-content__button md-icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>



<p align="center">
  <a href="https://github.com/ESAOpenSR/opensr-test"><img src="docs/images/logo.png" alt="header" width="45%"></a>
</p>

<p align="center">
    <em>
    A comprehensive benchmark for real-world Sentinel-2 imagery super-resolution
    </em>
</p>

<p align="center">
<a href='https://pypi.python.org/pypi/opensr-test'>
<img src='https://img.shields.io/pypi/v/opensr-test.svg' alt='PyPI' />
</a>
<a href='https://colab.research.google.com/drive/1wDD_d0RUnAZkJTf63_t9qULjPjtCmbuv?usp=sharing'>
<img src='https://colab.research.google.com/assets/colab-badge.svg' alt='COLAB' />
</a>
<a href="https://opensource.org/licenses/MIT" target="_blank">
    <img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License">
</a>
<a href="https://github.com/psf/black" target="_blank">
    <img src="https://img.shields.io/badge/code%20style-black-000000.svg" alt="Black">
</a>
<a href="https://pycqa.github.io/isort/" target="_blank">
    <img src="https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336" alt="isort">
</a>
</p>

<hr />
<p><strong>GitHub</strong>: <a href="https://github.com/ESAOpenSR/opensr-test">https://github.com/ESAOpenSR/opensr-test</a></p>
<p><strong>Documentation</strong>: <a href="https://esaopensr.github.io/opensr-test">https://esaopensr.github.io/opensr-test</a></p>
<p><strong>PyPI</strong>: <a href="https://pypi.org/project/opensr-test/">https://pypi.org/project/opensr-test/</a></p>
<p><strong>Paper</strong>: <a href="https://ieeexplore.ieee.org/document/10530998">https://ieeexplore.ieee.org/document/10530998</a></p>
<hr />
<h1 id="_1"></h1>
<h2 id="overview"><strong>Overview</strong></h2>
<p>Super-Resolution (SR) aims to improve satellite imagery ground sampling distance. However, two problems are common in the literature. First, most models are <strong>tested on synthetic data</strong>, raising doubts about their real-world applicability and performance. Second, traditional evaluation metrics such as PSNR, LPIPS, and SSIM are not designed to assess SR performance. These metrics fall short, especially in conditions involving changes in luminance or spatial misalignments - scenarios frequently encountered in real world.</p>
<p>To address these challenges, 'opensr-test' provides a fair approach for SR benchmark. We provide three datasets carefully crafted to minimize spatial and spectral misalignment. Besides, 'opensr-test' precisely assesses SR algorithm performance across three independent metrics groups that measure consistency, synthesis, and correctness.</p>
<p align="center">
  <img src="docs/images/diagram.png" alt="header">
</p>

<h2 id="how-to-use"><strong>How to use</strong></h2>
<p>The example below shows how to use <code>opensr-test</code> to benchmark your SR model.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">opensr_test</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">hr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">sr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>

<span class="n">metrics</span> <span class="o">=</span> <span class="n">opensr_test</span><span class="o">.</span><span class="n">Metrics</span><span class="p">()</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">hr</span><span class="o">=</span><span class="n">hr</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">{</span><span class="s1">&#39;reflectance&#39;</span><span class="p">:</span> <span class="mf">0.253</span><span class="p">,</span> <span class="s1">&#39;spectral&#39;</span><span class="p">:</span> <span class="mf">26.967</span><span class="p">,</span> <span class="s1">&#39;spatial&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;synthesis&#39;</span><span class="p">:</span> <span class="mf">0.2870</span><span class="p">,</span> <span class="s1">&#39;ha_percent&#39;</span><span class="p">:</span> <span class="mf">0.892</span><span class="p">,</span> <span class="s1">&#39;om_percent&#39;</span><span class="p">:</span> <span class="mf">0.0613</span><span class="p">,</span> <span class="s1">&#39;im_percent&#39;</span><span class="p">:</span> <span class="mf">0.04625</span><span class="p">}</span>
</code></pre></div>
<p>This model returns:</p>
<ul>
<li><strong>reflectance</strong>: How SR affects the reflectance norm of the LR image. By default, it uses the L1 distance. The lower the value, the better the reflectance consistency.</li>
</ul>
<ul>
<li><strong>spectral</strong>: How SR affects the spectral signature of the LR image. By default, it uses the spectral angle distance (SAD). The lower the value, the better the spectral consistency. The angles are in degrees.</li>
</ul>
<ul>
<li><strong>spatial</strong>: The spatial alignment between the SR and LR images. By default, it uses Phase Correlation Coefficient (PCC). Some SR models introduce spatial shift, which can be detected by this metric.</li>
</ul>
<ul>
<li><strong>synthesis</strong>: The high-frequency details introduced by the SR model. By default, it uses the L1 distance. The higher the value, the better the synthesis quality.</li>
</ul>
<ul>
<li><strong>ha_metric</strong>: The amount of hallucinations in the SR image. A hallucination is a detail (high-gradient) in the SR image that <strong>is not present in the HR image.</strong> The lower the value, the better the correctness of the SR image.</li>
</ul>
<ul>
<li><strong>om_metric</strong>: The amount of omissions in the SR image. An omission is a detail in the HR image that <strong>is not present in the SR image.</strong> The lower the value, the better the correctness of the SR image.</li>
</ul>
<ul>
<li><strong>im_metric</strong>: The amount of improvements in the SR image. An improvement is a detail in the SR image that <strong>is present in the HR image and not in the LR image.</strong> The higher the value, the better the correctness of the SR image.</li>
</ul>
<h2 id="benchmark"><strong>Benchmark</strong></h2>
<p>Benchmark comparison of SR models. Downward arrows (↓) denote metrics in which lower values are preferable, and upward arrows (↑) indicate that higher values reflect better performance. This table is an improved version of the one presented in the opensr-test paper, considering the latest version of the datasets and other distance metrics. To get the datasets used in the <a href="https://ieeexplore.ieee.org/document/10530998">original paper</a>, set the <code>dataset</code> parameter to <code>version=020</code> in the <code>opensr_test.load</code> function.</p>
<h3 id="clip"><strong>CLIP</strong></h3>
<p>We use the <a href="https://github.com/ChenDelong1999/RemoteCLIP"><strong>RemoteCLIP</strong></a> model to measure the distance between the LR, SR, and HR images. The parameters of the experiments are: <code>{"device": "cuda", "agg_method": "patch", "patch_size": 16, "correctness_distance": "clip"}</code>. This distance metric allows us to quantify the amount of semantic information introduced by the SR model. By comparing the three categories, we can assess the accuracy of the semantic information introduced.</p>
<table>
<thead>
<tr>
<th align="left">model</th>
<th align="left">reflectance ↓</th>
<th align="left">spectral ↓</th>
<th align="left">spatial ↓</th>
<th align="left">synthesis ↑</th>
<th align="left">ha_metric ↓</th>
<th align="left">om_metric ↓</th>
<th align="left">im_metric ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">ldm_baseline</td>
<td align="left">0.1239 ± 0.0405</td>
<td align="left">12.8441 ± 2.7508</td>
<td align="left">0.0717 ± 0.0683</td>
<td align="left">0.0409 ± 0.0290</td>
<td align="left">0.5963 ± 0.3055</td>
<td align="left">0.2327 ± 0.2238</td>
<td align="left">0.1710 ± 0.1435</td>
</tr>
<tr>
<td align="left">opensrmodel</td>
<td align="left">0.0076 ± 0.0046</td>
<td align="left">1.9739 ± 1.0507</td>
<td align="left">0.0118 ± 0.0108</td>
<td align="left">0.0194 ± 0.0120</td>
<td align="left">0.1052 ± 0.0590</td>
<td align="left">0.6927 ± 0.1343</td>
<td align="left">0.2021 ± 0.0854</td>
</tr>
<tr>
<td align="left">satlas</td>
<td align="left">0.1197 ± 0.0233</td>
<td align="left">15.1521 ± 2.9876</td>
<td align="left">0.2766 ± 0.0741</td>
<td align="left">0.0648 ± 0.0302</td>
<td align="left">0.6996 ± 0.2058</td>
<td align="left">0.0872 ± 0.0947</td>
<td align="left">0.2132 ± 0.1393</td>
</tr>
<tr>
<td align="left">sr4rs</td>
<td align="left">0.0979 ± 0.0509</td>
<td align="left">22.4905 ± 2.1168</td>
<td align="left">1.0099 ± 0.0439</td>
<td align="left">0.0509 ± 0.0237</td>
<td align="left">0.3099 ± 0.1704</td>
<td align="left">0.3486 ± 0.1753</td>
<td align="left">0.3415 ± 0.1042</td>
</tr>
<tr>
<td align="left">superimage</td>
<td align="left">0.0068 ± 0.0016</td>
<td align="left">1.8977 ± 1.1053</td>
<td align="left">0.0004 ± 0.0032</td>
<td align="left">0.0130 ± 0.0073</td>
<td align="left">0.0610 ± 0.0305</td>
<td align="left">0.8524 ± 0.0586</td>
<td align="left">0.0866 ± 0.0395</td>
</tr>
</tbody>
</table>
<h3 id="lpips"><strong>LPIPS</strong></h3>
<p>We use the <a href="https://github.com/richzhang/PerceptualSimilarity"><strong>LPIPS</strong></a> model to measure the distance between the LR, SR, and HR images. The parameters of the experiments are: <code>{"device": "cuda", "agg_method": "patch", "patch_size": 16, "correctness_distance": "lpips"}</code>. This distance metric allows us to quantify the amount of perceptual information introduced by the SR model. By comparing the three categories, we can assess the accuracy of the perceptual information introduced.</p>
<table>
<thead>
<tr>
<th align="left">model</th>
<th align="left">reflectance ↓</th>
<th align="left">spectral ↓</th>
<th align="left">spatial ↓</th>
<th align="left">synthesis ↑</th>
<th align="left">ha_metric ↓</th>
<th align="left">om_metric ↓</th>
<th align="left">im_metric ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">ldm_baseline</td>
<td align="left">0.1239 ± 0.0405</td>
<td align="left">12.8441 ± 2.7508</td>
<td align="left">0.0717 ± 0.0683</td>
<td align="left">0.0409 ± 0.0290</td>
<td align="left">0.4558 ± 0.2932</td>
<td align="left">0.3558 ± 0.2518</td>
<td align="left">0.1884 ± 0.1232</td>
</tr>
<tr>
<td align="left">opensrmodel</td>
<td align="left">0.0076 ± 0.0046</td>
<td align="left">1.9739 ± 1.0507</td>
<td align="left">0.0118 ± 0.0108</td>
<td align="left">0.0194 ± 0.0120</td>
<td align="left">0.0642 ± 0.0271</td>
<td align="left">0.6690 ± 0.1291</td>
<td align="left">0.2668 ± 0.1071</td>
</tr>
<tr>
<td align="left">satlas</td>
<td align="left">0.1197 ± 0.0233</td>
<td align="left">15.1521 ± 2.9876</td>
<td align="left">0.2766 ± 0.0741</td>
<td align="left">0.0648 ± 0.0302</td>
<td align="left">0.5999 ± 0.2182</td>
<td align="left">0.0588 ± 0.0552</td>
<td align="left">0.3413 ± 0.1858</td>
</tr>
<tr>
<td align="left">sr4rs</td>
<td align="left">0.0979 ± 0.0509</td>
<td align="left">22.4905 ± 2.1168</td>
<td align="left">1.0099 ± 0.0439</td>
<td align="left">0.0509 ± 0.0237</td>
<td align="left">0.3417 ± 0.1833</td>
<td align="left">0.1924 ± 0.1402</td>
<td align="left">0.4659 ± 0.1448</td>
</tr>
<tr>
<td align="left">superimage</td>
<td align="left">0.0068 ± 0.0016</td>
<td align="left">1.8977 ± 1.1053</td>
<td align="left">0.0004 ± 0.0032</td>
<td align="left">0.0130 ± 0.0073</td>
<td align="left">0.0357 ± 0.0200</td>
<td align="left">0.8844 ± 0.0391</td>
<td align="left">0.0800 ± 0.0301</td>
</tr>
</tbody>
</table>
<h3 id="normalized-difference-nd"><strong>Normalized Difference (ND)</strong></h3>
<p>We use the normalized difference (ND) distance to measure the distance between the LR, SR, and HR images. In contrast to L1 distance, ND is less sensitive to the magnitude of the reflectance values, providing a more fair detection of the hallucinations and omissions when assessing at the pixel level. The parameters of the experiments are: <code>{"device": "cuda", "agg_method": "patch", "patch_size": 1, "correctness_distance": "nd"}</code>. This distance metric allows us to quantify high-frequency details introduced by the SR model at pixel level. However, at pixel level, small changes in the reflectance values could introduce noise in the hallucination, omission, and improvement estimation.</p>
<table>
<thead>
<tr>
<th align="left">model</th>
<th align="left">reflectance ↓</th>
<th align="left">spectral ↓</th>
<th align="left">spatial ↓</th>
<th align="left">synthesis ↑</th>
<th align="left">ha_metric ↓</th>
<th align="left">om_metric ↓</th>
<th align="left">im_metric ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">ldm_baseline</td>
<td align="left">0.0505 ± 0.0161</td>
<td align="left">9.6923 ± 2.1742</td>
<td align="left">0.0715 ± 0.0679</td>
<td align="left">0.0285 ± 0.0307</td>
<td align="left">0.6067 ± 0.2172</td>
<td align="left">0.3088 ± 0.1786</td>
<td align="left">0.0845 ± 0.0428</td>
</tr>
<tr>
<td align="left">opensrmodel</td>
<td align="left">0.0031 ± 0.0018</td>
<td align="left">1.2632 ± 0.5878</td>
<td align="left">0.0114 ± 0.0111</td>
<td align="left">0.0068 ± 0.0044</td>
<td align="left">0.3431 ± 0.0738</td>
<td align="left">0.4593 ± 0.0781</td>
<td align="left">0.1976 ± 0.0328</td>
</tr>
<tr>
<td align="left">satlas</td>
<td align="left">0.0489 ± 0.0086</td>
<td align="left">12.1231 ± 3.1529</td>
<td align="left">0.2742 ± 0.0748</td>
<td align="left">0.0227 ± 0.0107</td>
<td align="left">0.8004 ± 0.0641</td>
<td align="left">0.1073 ± 0.0393</td>
<td align="left">0.0923 ± 0.0266</td>
</tr>
<tr>
<td align="left">sr4rs</td>
<td align="left">0.0396 ± 0.0198</td>
<td align="left">3.4044 ± 1.6882</td>
<td align="left">1.0037 ± 0.1520</td>
<td align="left">0.0177 ± 0.0083</td>
<td align="left">0.7274 ± 0.0840</td>
<td align="left">0.1637 ± 0.0572</td>
<td align="left">0.1089 ± 0.0292</td>
</tr>
<tr>
<td align="left">superimage</td>
<td align="left">0.0029 ± 0.0009</td>
<td align="left">1.5672 ± 1.0692</td>
<td align="left">0.0132 ± 0.1131</td>
<td align="left">0.0046 ± 0.0027</td>
<td align="left">0.2026 ± 0.0692</td>
<td align="left">0.6288 ± 0.0754</td>
<td align="left">0.1686 ± 0.0302</td>
</tr>
</tbody>
</table>
<p>To reproduce the results, check this <a href="https://colab.research.google.com/drive/1wDD_d0RUnAZkJTf63_t9qULjPjtCmbuv"><strong>Colab notebook</strong></a>.</p>
<h2 id="installation"><strong>Installation</strong></h2>
<p>Install the latest version from PyPI:</p>
<div class="highlight"><pre><span></span><code>pip install opensr-test
pip install opensr-test[perceptual] # Install to test perceptual metrics
</code></pre></div>
<p>Upgrade <code>opensr-test</code> by running:</p>
<div class="highlight"><pre><span></span><code>pip install -U opensr-test
</code></pre></div>
<p>Install the latest dev version from GitHub by running:</p>
<div class="highlight"><pre><span></span><code>pip install git+https://github.com/ESAOpenSR/opensr-test
</code></pre></div>
<h2 id="datasets"><strong>Datasets</strong></h2>
<p>The <code>opensr-test</code> package provides five datasets for benchmarking SR models. These datasets are carefully crafted to minimize spatial and spectral misalignment. See our Hugging Face repository for more details about the datasets. <a href="https://huggingface.co/datasets/isp-uv-es/opensr-test"><strong>https://huggingface.co/datasets/isp-uv-es/opensr-test</strong></a></p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Scale factor</th>
<th>Number of images</th>
<th>HR patch size</th>
</tr>
</thead>
<tbody>
<tr>
<td>NAIP</td>
<td>x4</td>
<td>62</td>
<td>484x484</td>
</tr>
<tr>
<td>SPOT</td>
<td>x4</td>
<td>9</td>
<td>512x512</td>
</tr>
<tr>
<td>Venµs</td>
<td>x2</td>
<td>59</td>
<td>256x256</td>
</tr>
<tr>
<td>SPAIN CROPS</td>
<td>x4</td>
<td>28</td>
<td>512x512</td>
</tr>
<tr>
<td>SPAIN URBAN</td>
<td>x4</td>
<td>20</td>
<td>512x512</td>
</tr>
</tbody>
</table>
<h3 id="naip-x4-scale-factor"><strong>NAIP (X4 scale factor)</strong></h3>
<p>The National Agriculture Imagery Program (NAIP) dataset is a high-resolution aerial imagery dataset that covers the continental United States. The dataset consists of 2.5m NAIP imagery captured in the visible and near-infrared spectrum (RGBNIR) and all Sentinel-2 L1C and L2A bands. The dataset focus in <strong>crop fields, forests, and bare soil areas</strong>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">opensr_test</span>

<span class="n">naip</span> <span class="o">=</span> <span class="n">opensr_test</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;naip&quot;</span><span class="p">)</span>
</code></pre></div>
<p align="center">
  <a href="https://github.com/ESAOpenSR/opensr-test"><img src="docs/images/NAIP.gif" alt="header" width="80%"></a>
</p>

<h3 id="spot-x4-scale-factor"><strong>SPOT (X4 scale factor)</strong></h3>
<p>The SPOT imagery were obtained from the worldstat dataset. The dataset consists of 2.5m SPOT imagery captured in the visible and near-infrared spectrum (RGBNIR) and all Sentinel-2 L1C and L2A bands. The dataset focus in <strong>urban areas, crop fields, and bare soil areas</strong>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">opensr_test</span>

<span class="n">spot</span> <span class="o">=</span> <span class="n">opensr_test</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;spot&quot;</span><span class="p">)</span>
</code></pre></div>
<p align="center">
  <a href="https://github.com/ESAOpenSR/opensr-test"><img src="docs/images/SPOT.gif" alt="header" width="80%"></a>
</p>

<h3 id="vens-x2-scale-factor"><strong>Venµs (X2 scale factor)</strong></h3>
<p>The Venµs images were obtained from the <a href="https://zenodo.org/records/6514159"><strong>Sen2Venµs dataset</strong></a>. The dataset consists of 5m Venµs imagery captured in the visible and near-infrared spectrum (RGBNIR) and all Sentinel-2 L1C and L2A bands. The dataset focus in <strong>crop fields, forests, urban areas, and bare soil areas</strong>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">opensr_test</span>

<span class="n">venus</span> <span class="o">=</span> <span class="n">opensr_test</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;venus&quot;</span><span class="p">)</span>
</code></pre></div>
<p align="center">
  <a href="https://github.com/ESAOpenSR/opensr-test"><img src="docs/images/VENUS.gif" alt="header" width="80%"></a>
</p>

<h3 id="spain-crops-x4-scale-factor"><strong>SPAIN CROPS (x4 scale factor)</strong></h3>
<p>The SPAIN CROPS dataset consists of 2.5m aerial imagery captured in the visible and near-infrared spectrum (RGBNIR) by the Spanish National Geographic Institute (IGN). The dataset includes all Sentinel-2 L1C and L2A bands. The dataset focus in <strong>crop fields and forests</strong>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">opensr_test</span>

<span class="n">spain_crops</span> <span class="o">=</span> <span class="n">opensr_test</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;spain_crops&quot;</span><span class="p">)</span>
</code></pre></div>
<p align="center">
  <a href="https://github.com/ESAOpenSR/opensr-test"><img src="docs/images/SPAIN_CROPS.gif" alt="header" width="80%"></a>
</p>

<h3 id="spain-urban-x4-scale-factor"><strong>SPAIN URBAN (x4 scale factor)</strong></h3>
<p>The SPAIN URBAN dataset consists of 2.5m aerial imagery captured in the visible and near-infrared spectrum (RGBNIR) by the Spanish National Geographic Institute (IGN). The dataset includes all Sentinel-2 L1C and L2A bands. The dataset focus in <strong>urban areas</strong>.</p>
<div class="highlight"><pre><span></span><code><span class="n">spain_urban</span> <span class="o">=</span> <span class="n">opensr_test</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;spain_urban&quot;</span><span class="p">)</span>
</code></pre></div>
<p align="center">
  <a href="https://github.com/ESAOpenSR/opensr-test"><img src="docs/images/SPAIN_URBAN.gif" alt="header" width="80%"></a>
</p>

<h2 id="examples"><strong>Examples</strong></h2>
<p>The following examples show how to use <code>opensr-test</code> to benchmark SR models.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Framework</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>SR4RS</td>
<td>TensorFlow</td>
<td><a href="https://huggingface.co/isp-uv-es/superIX/blob/main/sr4rs/run.py"><img alt="Model on HF" src="https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-md.svg" /></a></td>
</tr>
<tr>
<td>SuperImage</td>
<td>PyTorch</td>
<td><a href="https://huggingface.co/isp-uv-es/superIX/blob/main/superimage/run.py"><img alt="Model on HF" src="https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-md.svg" /></a></td>
</tr>
<tr>
<td>LDMSuperResolutionPipeline</td>
<td>Diffuser</td>
<td><a href="https://huggingface.co/isp-uv-es/superIX/blob/main/ldm_baseline/run.py"><img alt="Model on HF" src="https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-md.svg" /></a></td>
</tr>
<tr>
<td>opensr-model</td>
<td>Pytorch</td>
<td><a href="https://huggingface.co/isp-uv-es/superIX/blob/main/opensrmodel/run.py"><img alt="Model on HF" src="https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-md.svg" /></a></td>
</tr>
<tr>
<td>EvoLand</td>
<td>Pytorch</td>
<td><a href="https://huggingface.co/isp-uv-es/superIX/blob/main/evoland/run.py"><img alt="Model on HF" src="https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-md.svg" /></a></td>
</tr>
<tr>
<td>SWIN2-MOSE</td>
<td>Pytorch</td>
<td><a href="https://huggingface.co/isp-uv-es/superIX/blob/main/swin2_mose/run.py"><img alt="Model on HF" src="https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-md.svg" /></a></td>
</tr>
</tbody>
</table>
<h2 id="visualizations"><strong>Visualizations</strong></h2>
<p>The <code>opensr-test</code> package provides a set of visualizations to help you understand the performance of your SR model.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">opensr_test</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">super_image</span> <span class="kn">import</span> <span class="n">HanModel</span>

<span class="c1"># Define the SR model</span>
<span class="n">srmodel</span> <span class="o">=</span> <span class="n">HanModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;eugenesiow/han&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">srmodel</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Load the data</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">opensr_test</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;spot&quot;</span><span class="p">)</span>
<span class="n">lr</span><span class="p">,</span> <span class="n">hr</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;L2A&quot;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;HRharm&quot;</span><span class="p">]</span>

<span class="c1"># Define the benchmark experiment</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">opensr_test</span><span class="o">.</span><span class="n">Config</span><span class="p">()</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">opensr_test</span><span class="o">.</span><span class="n">Metrics</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Define the image to be tested</span>
<span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">lr_img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">lr</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span> <span class="o">/</span> <span class="mi">10000</span>
<span class="n">hr_img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">hr</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span> <span class="o">/</span> <span class="mi">10000</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">sr_img</span> <span class="o">=</span> <span class="n">srmodel</span><span class="p">(</span><span class="n">lr_img</span><span class="p">[</span><span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

<span class="c1"># Compute the metrics</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">lr_img</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr_img</span><span class="p">,</span> <span class="n">hr</span><span class="o">=</span><span class="n">hr_img</span>
<span class="p">)</span>
</code></pre></div>
<p>Now, we can visualize the results using the <code>opensr_test.plot_*</code> module. To display the triplets LR, SR and HR images:</p>
<div class="highlight"><pre><span></span><code><span class="n">metrics</span><span class="o">.</span><span class="n">plot_triplets</span><span class="p">()</span>
</code></pre></div>
<p align="center">
  <img src="docs/images/img_01.gif">
</p>

<p>Display the summary of the metrics. The plot shows:
  - LR: Low Resolution image 
  - LRdown: Downsampled Low Resolution image using bilinear interpolation and triangular antialiasing filter.
  - SR: Super-Resolved image.
  - SRharm: Harmonized super-resolution image.
  - HR: High Resolution image.
  - Reflectance Consistency: Reflectance consistency between the LR and HR images.
  - Spectral Consistency: Spectral consistency between the LR and HR images.
  - Distance normalized to the Omissiom, Hallucination, and Improvement spaces.</p>
<div class="highlight"><pre><span></span><code><span class="n">metrics</span><span class="o">.</span><span class="n">plot_summary</span><span class="p">()</span>
</code></pre></div>
<p align="center">
  <img src="docs/images/img_02.gif">
</p>

<p>Display the correctness of the SR image. The blue color represents the pixels closer to the improvement space (HR), green pixels are closer to the omission space (LR), and red pixels are closer to the hallucination space (neither in HR nor LR). The distance SR-LR and SR-HR are normalized to the LR-HR distance that is independent of the SR model. Threfore a pixel or patch with a improvement distance (<span class="arithmatex">\(d_{im}\)</span>) of 3 means that the SR is further to the HR in 3 LR-HR units. The same applies to the omission distance.</p>
<div class="highlight"><pre><span></span><code><span class="n">metrics</span><span class="o">.</span><span class="n">plot_tc</span><span class="p">()</span>
</code></pre></div>
<p align="center">
  <img src="docs/images/img_03.gif">
</p>

<p>Display the histogram of the distances before and after the normalization:</p>
<div class="highlight"><pre><span></span><code><span class="n">metrics</span><span class="o">.</span><span class="n">plot_stats</span><span class="p">()</span>
</code></pre></div>
<p align="center">
  <img src="docs/images/img_04.gif">
</p>

<p>Display a ternary plot of the metrics:</p>
<div class="highlight"><pre><span></span><code><span class="n">metrics</span><span class="o">.</span><span class="n">plot_ternary</span><span class="p">()</span>
</code></pre></div>
<p align="center">
  <img src="docs/images/img_05.gif">
</p>

<h2 id="deeper-understanding"><strong>Deeper understanding</strong></h2>
<p>Explore the <a href="https://esaopensr.github.io/opensr-test/docs/API/config_pydantic.html"><strong>API</strong></a> section for more details about personalizing your benchmark experiments.</p>
<h2 id="citation"><strong>Citation</strong></h2>
<p>If you use <code>opensr-test</code> in your research, please cite our paper:</p>
<div class="highlight"><pre><span></span><code>@ARTICLE{10530998,
  author={Aybar, Cesar and Montero, David and Donike, Simon and Kalaitzis, Freddie and Gómez-Chova, Luis},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={A Comprehensive Benchmark for Optical Remote Sensing Image Super-Resolution}, 
  year={2024},
  volume={21},
  number={},
  pages={1-5},
  keywords={Measurement;Remote sensing;Spatial resolution;Superresolution;Reflectivity;Protocols;Inspection;Benchmarking;datasets;deep learning;NAIP;Sentinel-2 (S2);SPOT;super-resolution (SR)},
  doi={10.1109/LGRS.2024.3401394}}
</code></pre></div>
<h2 id="acknowledgements"><strong>Acknowledgements</strong></h2>
<p>This work was make with the support of the European Space Agency (ESA) under the project “Explainable AI: application to trustworthy super-resolution (OpenSR)”. Cesar Aybar acknowledges support by the National Council of Science, Technology, and Technological Innovation (CONCYTEC, Peru) through the “PROYECTOS DE INVESTIGACIÓN BÁSICA – 2023-01” program with contract number PE501083135-2023-PROCIENCIA. Luis Gómez-Chova acknowledges support from the Spanish Ministry of Science and Innovation (project PID2019-109026RB-I00 funded by MCIN/AEI/10.13039/501100011033).</p>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
      
        
        <a href="docs/CONTRIBUTING.html" class="md-footer__link md-footer__link--next" aria-label="Next: Contributing" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Contributing
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": ".", "features": ["navigation.instant", "navigation.tabs", "navigation.top", "navigation.expand", "navigation.indexes", "header.autohide"], "search": "assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.3a4b43e5.min.js"></script>
      
        <script src="javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>